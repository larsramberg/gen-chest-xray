Lmod has detected the following error: Your site prevents the automatic
swapping of modules with same name. You must explicitly unload the loaded
version of "GCC/11.3.0" before you can load the new one. Use swap to do this:

   $ module swap GCC/11.3.0 GCC/12.3.0

Alternatively, you can set the environment variable
LMOD_DISABLE_SAME_NAME_AUTOSWAP to "no" to re-enable same name autoswapping.

While processing the following module(s):
    Module fullname              Module Filename
    ---------------              ---------------
    gfbf/2023a                   /cluster/apps/eb/modules/all/gfbf/2023a.lua
    matplotlib/3.7.2-gfbf-2023a  /cluster/apps/eb/modules/all/matplotlib/3.7.2-gfbf-2023a.lua

Starting training
Training on device: NVIDIA A100-PCIE-40GB
  0%|          | 0/100 [00:00<?, ?it/s]Starting epoch 1 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
  1%|          | 36/4905 [01:40<3:46:59,  2.80s/it][A
  9%|â–‰         | 430/4905 [03:21<30:23,  2.45it/s] [A
 16%|â–ˆâ–‹        | 799/4905 [05:01<22:46,  3.01it/s][A
 24%|â–ˆâ–ˆâ–       | 1195/4905 [06:41<18:17,  3.38it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1586/4905 [08:22<15:29,  3.57it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1950/4905 [10:02<13:43,  3.59it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2338/4905 [11:43<11:39,  3.67it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2759/4905 [13:23<09:18,  3.84it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3137/4905 [15:03<07:42,  3.82it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3527/4905 [16:44<05:58,  3.84it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3914/4905 [18:24<04:17,  3.84it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4264/4905 [20:04<02:51,  3.74it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4620/4905 [21:44<01:17,  3.68it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.5616060422360898
Last average training loss at batch 1000 out of 4905: 1.4761083620637656
Last average training loss at batch 1500 out of 4905: 1.5283763992637396
Last average training loss at batch 2000 out of 4905: 1.6075337989926337
Last average training loss at batch 2500 out of 4905: 1.7514838565886022
Last average training loss at batch 3000 out of 4905: 1.8615606322288514
Last average training loss at batch 3500 out of 4905: 1.9173496881574392
Last average training loss at batch 4000 out of 4905: 1.9728636790663003
Last average training loss at batch 4500 out of 4905: 1.6367472510188819
Last average training loss at batch 4905 out of 4905: 0.15159690282274338

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 271/702 [01:20<02:08,  3.35it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 579/702 [02:42<00:34,  3.60it/s][A
                                                 [A  1%|          | 1/100 [26:20<43:27:48, 1580.49s/it]New best average validation loss of 1.6534227132797241 with accuracy 0.5972522084560171 during epoch 1
Starting epoch 2 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
  7%|â–‹         | 329/4905 [01:40<23:11,  3.29it/s][A
 13%|â–ˆâ–Ž        | 638/4905 [03:20<22:25,  3.17it/s][A
 19%|â–ˆâ–‰        | 935/4905 [05:00<21:34,  3.07it/s][A
 25%|â–ˆâ–ˆâ–       | 1225/4905 [06:41<20:30,  2.99it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 1531/4905 [08:22<18:41,  3.01it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1871/4905 [10:02<16:06,  3.14it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2170/4905 [11:42<14:45,  3.09it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2447/4905 [13:22<13:42,  2.99it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2800/4905 [15:02<11:07,  3.16it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3140/4905 [16:42<09:06,  3.23it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3497/4905 [18:22<07:02,  3.33it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3853/4905 [20:03<05:09,  3.40it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4215/4905 [21:44<03:19,  3.45it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4558/4905 [23:25<01:40,  3.44it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.5233047100752592
Last average training loss at batch 1000 out of 4905: 1.4586047718822956
Last average training loss at batch 1500 out of 4905: 1.5149753711223601
Last average training loss at batch 2000 out of 4905: 1.596690790757537
Last average training loss at batch 2500 out of 4905: 1.736286088988185
Last average training loss at batch 3000 out of 4905: 1.8467887978702784
Last average training loss at batch 3500 out of 4905: 1.8967766360640526
Last average training loss at batch 4000 out of 4905: 1.9589714229106903
Last average training loss at batch 4500 out of 4905: 1.621751800686121
Last average training loss at batch 4905 out of 4905: 0.15036339816788286

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 255/702 [01:20<02:20,  3.19it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 521/702 [02:41<00:55,  3.24it/s][A
                                                 [A  2%|â–         | 2/100 [55:07<45:22:23, 1666.77s/it]New best average validation loss of 1.640733003616333 with accuracy 0.6263064080955495 during epoch 2
Starting epoch 3 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
  8%|â–Š         | 380/4905 [01:40<19:51,  3.80it/s][A
 15%|â–ˆâ–Œ        | 742/4905 [03:20<18:53,  3.67it/s][A
 24%|â–ˆâ–ˆâ–       | 1191/4905 [05:00<15:18,  4.05it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1636/4905 [06:41<12:58,  4.20it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2136/4905 [08:21<10:16,  4.49it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2602/4905 [10:01<08:26,  4.55it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3139/4905 [11:41<06:06,  4.81it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3662/4905 [13:23<04:12,  4.91it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4155/4905 [15:05<02:33,  4.89it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4685/4905 [16:46<00:43,  5.00it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.5112739000618458
Last average training loss at batch 1000 out of 4905: 1.449091573819518
Last average training loss at batch 1500 out of 4905: 1.5033813367038966
Last average training loss at batch 2000 out of 4905: 1.5830681135207414
Last average training loss at batch 2500 out of 4905: 1.7231590687930585
Last average training loss at batch 3000 out of 4905: 1.8338482759445907
Last average training loss at batch 3500 out of 4905: 1.8870296939313411
Last average training loss at batch 4000 out of 4905: 1.9443708559274673
Last average training loss at batch 4500 out of 4905: 1.6097977433651687
Last average training loss at batch 4905 out of 4905: 0.14944968751871865

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 316/702 [01:20<01:37,  3.94it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 605/702 [02:40<00:25,  3.74it/s][A
                                                 [A  3%|â–Ž         | 3/100 [1:15:44<39:37:17, 1470.49s/it]Average loss of 1.6426324844360352 is not an improvement over 1.640733003616333. New model will not be saved
Starting epoch 4 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
  7%|â–‹         | 347/4905 [01:40<22:00,  3.45it/s][A
 14%|â–ˆâ–        | 697/4905 [03:22<20:20,  3.45it/s][A
 21%|â–ˆâ–ˆâ–       | 1043/4905 [05:02<18:38,  3.45it/s][A
 29%|â–ˆâ–ˆâ–Š       | 1407/4905 [06:42<16:32,  3.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1746/4905 [08:22<15:08,  3.48it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2112/4905 [10:02<13:09,  3.54it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2541/4905 [11:42<10:25,  3.78it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2997/4905 [13:23<07:54,  4.02it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3447/4905 [15:03<05:49,  4.17it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3905/4905 [16:44<03:53,  4.28it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4367/4905 [18:24<02:02,  4.38it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4814/4905 [20:04<00:20,  4.41it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.502908985480666
Last average training loss at batch 1000 out of 4905: 1.4412044667452575
Last average training loss at batch 1500 out of 4905: 1.495253516435623
Last average training loss at batch 2000 out of 4905: 1.5754455433934926
Last average training loss at batch 2500 out of 4905: 1.7133884938955306
Last average training loss at batch 3000 out of 4905: 1.8251934293955565
Last average training loss at batch 3500 out of 4905: 1.872036878809333
Last average training loss at batch 4000 out of 4905: 1.9345214034020901
Last average training loss at batch 4500 out of 4905: 1.6018952208906412
Last average training loss at batch 4905 out of 4905: 0.14847385726572906

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 327/702 [01:20<01:31,  4.09it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 667/702 [02:40<00:08,  4.18it/s][A
                                                 [A  4%|â–         | 4/100 [1:39:05<38:28:55, 1443.08s/it]New best average validation loss of 1.635375738143921 with accuracy 0.656903521459447 during epoch 4
Starting epoch 5 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
  8%|â–Š         | 410/4905 [01:40<18:18,  4.09it/s][A
 15%|â–ˆâ–        | 719/4905 [03:20<19:55,  3.50it/s][A
 22%|â–ˆâ–ˆâ–       | 1080/4905 [05:00<17:57,  3.55it/s][A
 28%|â–ˆâ–ˆâ–Š       | 1393/4905 [06:40<17:17,  3.38it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1734/4905 [08:21<15:37,  3.38it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2084/4905 [10:01<13:44,  3.42it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2451/4905 [11:41<11:41,  3.50it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2762/4905 [13:22<10:35,  3.37it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3151/4905 [15:02<08:16,  3.53it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3534/4905 [16:42<06:18,  3.62it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3865/4905 [18:22<04:55,  3.52it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4216/4905 [20:02<03:15,  3.52it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4527/4905 [21:44<01:51,  3.38it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4905/4905 [23:45<00:00,  3.29it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.492964171975851
Last average training loss at batch 1000 out of 4905: 1.430230560734868
Last average training loss at batch 1500 out of 4905: 1.4866211684644222
Last average training loss at batch 2000 out of 4905: 1.5634301881343127
Last average training loss at batch 2500 out of 4905: 1.7052248686552047
Last average training loss at batch 3000 out of 4905: 1.8138625900298357
Last average training loss at batch 3500 out of 4905: 1.8611947464346885
Last average training loss at batch 4000 out of 4905: 1.9263637073636055
Last average training loss at batch 4500 out of 4905: 1.5945619289129973
Last average training loss at batch 4905 out of 4905: 0.14760416849126146

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 272/702 [01:20<02:06,  3.40it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 524/702 [02:41<00:55,  3.21it/s][A
                                                 [A  5%|â–Œ         | 5/100 [2:06:43<40:07:21, 1520.43s/it]New best average validation loss of 1.6291018724441528 with accuracy 0.6515994097652963 during epoch 5
Starting epoch 6 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
  5%|â–Œ         | 265/4905 [01:40<29:13,  2.65it/s][A
 10%|â–ˆ         | 500/4905 [03:22<30:02,  2.44it/s][A
 17%|â–ˆâ–‹        | 813/4905 [05:02<24:47,  2.75it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1111/4905 [06:43<22:21,  2.83it/s][A
 28%|â–ˆâ–ˆâ–Š       | 1395/4905 [08:48<22:31,  2.60it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 1687/4905 [10:28<19:52,  2.70it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1934/4905 [12:09<18:52,  2.62it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2276/4905 [13:50<15:18,  2.86it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2593/4905 [15:30<13:03,  2.95it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2913/4905 [17:11<10:58,  3.02it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3316/4905 [18:51<07:57,  3.33it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3725/4905 [20:31<05:32,  3.55it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4137/4905 [22:12<03:26,  3.72it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4541/4905 [23:52<01:35,  3.81it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4863614729791879
Last average training loss at batch 1000 out of 4905: 1.4229161481559276
Last average training loss at batch 1500 out of 4905: 1.4791412061154843
Last average training loss at batch 2000 out of 4905: 1.5585764185488225
Last average training loss at batch 2500 out of 4905: 1.6951338127404452
Last average training loss at batch 3000 out of 4905: 1.80695372338593
Last average training loss at batch 3500 out of 4905: 1.8586819598376751
Last average training loss at batch 4000 out of 4905: 1.919853041291237
Last average training loss at batch 4500 out of 4905: 1.589521051093936
Last average training loss at batch 4905 out of 4905: 0.14681956283786124

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 302/702 [01:20<01:46,  3.75it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 626/702 [02:40<00:19,  3.92it/s][A
                                                 [A  6%|â–Œ         | 6/100 [2:35:13<41:23:23, 1585.14s/it]New best average validation loss of 1.6236716508865356 with accuracy 0.6631721315428939 during epoch 6
Starting epoch 7 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
  8%|â–Š         | 379/4905 [01:40<20:00,  3.77it/s][A
 16%|â–ˆâ–Œ        | 791/4905 [03:20<17:15,  3.97it/s][A
 25%|â–ˆâ–ˆâ–       | 1206/4905 [05:01<15:15,  4.04it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1568/4905 [06:41<14:21,  3.87it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1984/4905 [08:21<12:14,  3.98it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2347/4905 [10:01<11:03,  3.86it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2707/4905 [11:41<09:42,  3.77it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3097/4905 [13:22<07:54,  3.81it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3507/4905 [15:02<05:58,  3.90it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3909/4905 [16:42<04:13,  3.93it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4308/4905 [18:22<02:31,  3.95it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4729/4905 [20:02<00:43,  4.03it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4798951003551484
Last average training loss at batch 1000 out of 4905: 1.416768013253808
Last average training loss at batch 1500 out of 4905: 1.4745910438597203
Last average training loss at batch 2000 out of 4905: 1.5535028055757285
Last average training loss at batch 2500 out of 4905: 1.6868646447062492
Last average training loss at batch 3000 out of 4905: 1.7946279707103967
Last average training loss at batch 3500 out of 4905: 1.842948405891657
Last average training loss at batch 4000 out of 4905: 1.9094756377637387
Last average training loss at batch 4500 out of 4905: 1.5820272975713014
Last average training loss at batch 4905 out of 4905: 0.14556200603644054

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:15<?, ?it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 303/702 [01:20<01:45,  3.78it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 587/702 [02:41<00:31,  3.62it/s][A
                                                 [A  7%|â–‹         | 7/100 [2:59:23<39:48:27, 1540.94s/it]Average loss of 1.6237781047821045 is not an improvement over 1.6236716508865356. New model will not be saved
Starting epoch 8 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
  6%|â–‹         | 317/4905 [01:42<24:44,  3.09it/s][A
 13%|â–ˆâ–Ž        | 639/4905 [03:22<22:27,  3.16it/s][A
 19%|â–ˆâ–‰        | 944/4905 [05:02<21:12,  3.11it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1259/4905 [06:43<19:28,  3.12it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1587/4905 [08:23<17:24,  3.18it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1910/4905 [10:03<15:38,  3.19it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2174/4905 [11:44<15:08,  3.01it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2451/4905 [13:24<13:58,  2.93it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2760/4905 [15:06<12:03,  2.96it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2983/4905 [16:46<11:41,  2.74it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3275/4905 [18:26<09:44,  2.79it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3598/4905 [20:06<07:27,  2.92it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3912/4905 [21:46<05:32,  2.99it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4287/4905 [23:26<03:12,  3.22it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4658/4905 [25:07<01:13,  3.36it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4721637346893548
Last average training loss at batch 1000 out of 4905: 1.4088189357966185
Last average training loss at batch 1500 out of 4905: 1.4648718689084053
Last average training loss at batch 2000 out of 4905: 1.5421864254176616
Last average training loss at batch 2500 out of 4905: 1.6806663426160813
Last average training loss at batch 3000 out of 4905: 1.7852039075642825
Last average training loss at batch 3500 out of 4905: 1.8323041259795427
Last average training loss at batch 4000 out of 4905: 1.9039409257173538
Last average training loss at batch 4500 out of 4905: 1.5773697565048932
Last average training loss at batch 4905 out of 4905: 0.14492154962608209

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 248/702 [01:20<02:26,  3.10it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 539/702 [02:40<00:47,  3.42it/s][A
                                                 [A  8%|â–Š         | 8/100 [3:28:58<41:16:56, 1615.39s/it]New best average validation loss of 1.6194359064102173 with accuracy 0.6993107784083588 during epoch 8
Starting epoch 9 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
  7%|â–‹         | 332/4905 [01:42<23:32,  3.24it/s][A
 14%|â–ˆâ–        | 678/4905 [03:22<20:57,  3.36it/s][A
 21%|â–ˆâ–ˆ        | 1038/4905 [05:02<18:35,  3.47it/s][A
 29%|â–ˆâ–ˆâ–Š       | 1402/4905 [06:43<16:31,  3.53it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1764/4905 [08:23<14:41,  3.56it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2131/4905 [10:04<12:54,  3.58it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2513/4905 [11:44<10:53,  3.66it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2958/4905 [13:24<08:17,  3.91it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3393/4905 [15:05<06:14,  4.04it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3839/4905 [16:46<04:16,  4.16it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4250/4905 [18:26<02:38,  4.14it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4738/4905 [20:06<00:38,  4.37it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4632204792946577
Last average training loss at batch 1000 out of 4905: 1.4037573936730623
Last average training loss at batch 1500 out of 4905: 1.4605241994857787
Last average training loss at batch 2000 out of 4905: 1.5318958615064622
Last average training loss at batch 2500 out of 4905: 1.6703882555663585
Last average training loss at batch 3000 out of 4905: 1.7731470404714347
Last average training loss at batch 3500 out of 4905: 1.823701087474823
Last average training loss at batch 4000 out of 4905: 1.9001710913777352
Last average training loss at batch 4500 out of 4905: 1.5695463265180587
Last average training loss at batch 4905 out of 4905: 0.14433584032498611

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 436/702 [01:20<00:48,  5.43it/s][A
                                                 [A  9%|â–‰         | 9/100 [3:51:58<38:58:27, 1541.84s/it]Average loss of 1.61981999874115 is not an improvement over 1.6194359064102173. New model will not be saved
Starting epoch 10 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
  9%|â–‰         | 441/4905 [01:40<16:52,  4.41it/s][A
 18%|â–ˆâ–Š        | 896/4905 [03:20<14:54,  4.48it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1314/4905 [05:06<14:10,  4.22it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1795/4905 [06:46<11:38,  4.45it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2318/4905 [08:26<09:07,  4.73it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2836/4905 [10:06<07:03,  4.88it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3101/4905 [11:46<07:14,  4.15it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3439/4905 [13:26<06:15,  3.91it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3971/4905 [15:07<03:34,  4.35it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4505/4905 [16:47<01:26,  4.65it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4579486627727747
Last average training loss at batch 1000 out of 4905: 1.3982735843211413
Last average training loss at batch 1500 out of 4905: 1.4555274156481028
Last average training loss at batch 2000 out of 4905: 1.5270527273416519
Last average training loss at batch 2500 out of 4905: 1.6640720396786928
Last average training loss at batch 3000 out of 4905: 1.7656571028530599
Last average training loss at batch 3500 out of 4905: 1.8157218595743179
Last average training loss at batch 4000 out of 4905: 1.8929991767704486
Last average training loss at batch 4500 out of 4905: 1.5608522450625897
Last average training loss at batch 4905 out of 4905: 0.14350738690655288

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 438/702 [01:20<00:48,  5.47it/s][A
                                                 [A 10%|â–ˆ         | 10/100 [4:12:15<36:02:07, 1441.42s/it]Average loss of 1.6197501420974731 is not an improvement over 1.6194359064102173. New model will not be saved
Starting epoch 11 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
 10%|â–‰         | 483/4905 [01:40<15:16,  4.83it/s][A
 20%|â–ˆâ–ˆ        | 991/4905 [03:20<13:06,  4.97it/s][A
 30%|â–ˆâ–ˆâ–‰       | 1454/4905 [05:00<11:56,  4.82it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2004/4905 [06:40<09:30,  5.08it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2521/4905 [08:20<07:46,  5.11it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2999/4905 [10:00<06:21,  5.00it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3575/4905 [11:40<04:13,  5.25it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4162/4905 [13:20<02:16,  5.44it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4718/4905 [15:00<00:34,  5.48it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4518460771292447
Last average training loss at batch 1000 out of 4905: 1.3946631447523832
Last average training loss at batch 1500 out of 4905: 1.4505666639357806
Last average training loss at batch 2000 out of 4905: 1.5227835002988577
Last average training loss at batch 2500 out of 4905: 1.65798337200284
Last average training loss at batch 3000 out of 4905: 1.7570801419466735
Last average training loss at batch 3500 out of 4905: 1.809502043440938
Last average training loss at batch 4000 out of 4905: 1.8857842980325221
Last average training loss at batch 4500 out of 4905: 1.5505519132614136
Last average training loss at batch 4905 out of 4905: 0.1428689674104998

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 425/702 [01:20<00:52,  5.26it/s][A
                                                 [A 11%|â–ˆ         | 11/100 [4:30:05<32:49:35, 1327.82s/it]Average loss of 1.6242612600326538 is not an improvement over 1.6194359064102173. New model will not be saved
Starting epoch 12 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
 11%|â–ˆ         | 546/4905 [01:42<13:36,  5.34it/s][A
 22%|â–ˆâ–ˆâ–       | 1085/4905 [03:22<11:51,  5.37it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1575/4905 [05:02<10:46,  5.15it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2065/4905 [06:42<09:22,  5.05it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2588/4905 [08:23<07:34,  5.09it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3104/4905 [10:03<05:52,  5.11it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3595/4905 [11:43<04:19,  5.05it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4086/4905 [13:23<02:43,  5.00it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4596/4905 [15:04<01:01,  5.03it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4472037727981806
Last average training loss at batch 1000 out of 4905: 1.3902178368717433
Last average training loss at batch 1500 out of 4905: 1.447864688143134
Last average training loss at batch 2000 out of 4905: 1.518420423924923
Last average training loss at batch 2500 out of 4905: 1.6584538692682982
Last average training loss at batch 3000 out of 4905: 1.7533359006643294
Last average training loss at batch 3500 out of 4905: 1.8067497428059578
Last average training loss at batch 4000 out of 4905: 1.8804676169455052
Last average training loss at batch 4500 out of 4905: 1.5488915513455868
Last average training loss at batch 4905 out of 4905: 0.1423501675980779

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 372/702 [01:20<01:11,  4.61it/s][A
                                                 [A 12%|â–ˆâ–        | 12/100 [4:48:46<30:55:04, 1264.82s/it]Average loss of 1.623165488243103 is not an improvement over 1.6194359064102173. New model will not be saved
Starting epoch 13 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:18<?, ?it/s][A
 10%|â–ˆ         | 505/4905 [01:40<14:31,  5.05it/s][A
 20%|â–ˆâ–‰        | 963/4905 [03:20<13:45,  4.77it/s][A
 30%|â–ˆâ–ˆâ–‰       | 1467/4905 [05:00<11:42,  4.89it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1989/4905 [06:40<09:40,  5.02it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2534/4905 [08:20<07:38,  5.17it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3043/4905 [10:00<06:01,  5.14it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3618/4905 [11:40<04:00,  5.34it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4196/4905 [13:20<02:09,  5.47it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4711/4905 [15:00<00:36,  5.37it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4461153552234174
Last average training loss at batch 1000 out of 4905: 1.3886975311934948
Last average training loss at batch 1500 out of 4905: 1.4419672779738903
Last average training loss at batch 2000 out of 4905: 1.5097535076737403
Last average training loss at batch 2500 out of 4905: 1.6484661851227282
Last average training loss at batch 3000 out of 4905: 1.7491053648740054
Last average training loss at batch 3500 out of 4905: 1.7980916213840246
Last average training loss at batch 4000 out of 4905: 1.8774074493944646
Last average training loss at batch 4500 out of 4905: 1.5441118167638779
Last average training loss at batch 4905 out of 4905: 0.1419776647083013

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:20<?, ?it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 460/702 [01:20<00:42,  5.72it/s][A
                                                 [A 13%|â–ˆâ–Ž        | 13/100 [5:06:34<29:07:34, 1205.22s/it]Average loss of 1.6227682828903198 is not an improvement over 1.6194359064102173. New model will not be saved
Starting epoch 14 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
 11%|â–ˆ         | 546/4905 [01:40<13:21,  5.44it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1124/4905 [03:20<11:10,  5.64it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1772/4905 [05:00<08:40,  6.02it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2373/4905 [06:40<07:00,  6.01it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3000/4905 [08:20<05:12,  6.11it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3663/4905 [10:00<03:17,  6.28it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4309/4905 [11:40<01:34,  6.34it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4422199708968402
Last average training loss at batch 1000 out of 4905: 1.3801833009421824
Last average training loss at batch 1500 out of 4905: 1.4343941719830036
Last average training loss at batch 2000 out of 4905: 1.5084082830250263
Last average training loss at batch 2500 out of 4905: 1.6467562916129828
Last average training loss at batch 3000 out of 4905: 1.7415680083483458
Last average training loss at batch 3500 out of 4905: 1.7946574349850417
Last average training loss at batch 4000 out of 4905: 1.873109149813652
Last average training loss at batch 4500 out of 4905: 1.5377952692508698
Last average training loss at batch 4905 out of 4905: 0.141746628176607

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 472/702 [01:20<00:39,  5.90it/s][A
                                                 [A 14%|â–ˆâ–        | 14/100 [5:21:41<26:38:21, 1115.13s/it]New best average validation loss of 1.6154884099960327 with accuracy 0.6905663296719066 during epoch 14
Starting epoch 15 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 12%|â–ˆâ–        | 570/4905 [01:40<12:41,  5.69it/s][A
 25%|â–ˆâ–ˆâ–       | 1208/4905 [03:20<10:06,  6.09it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1882/4905 [05:00<07:53,  6.39it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2531/4905 [06:40<06:09,  6.43it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3167/4905 [08:20<04:31,  6.40it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3713/4905 [10:00<03:16,  6.08it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4363/4905 [11:40<01:27,  6.22it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4352608381062746
Last average training loss at batch 1000 out of 4905: 1.3785402533262967
Last average training loss at batch 1500 out of 4905: 1.4313069556951523
Last average training loss at batch 2000 out of 4905: 1.5009258191585542
Last average training loss at batch 2500 out of 4905: 1.6386540956497193
Last average training loss at batch 3000 out of 4905: 1.7408088037371636
Last average training loss at batch 3500 out of 4905: 1.7866452527195216
Last average training loss at batch 4000 out of 4905: 1.86898160469532
Last average training loss at batch 4500 out of 4905: 1.5324836951345204
Last average training loss at batch 4905 out of 4905: 0.14114246045929454

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 548/702 [01:20<00:22,  6.84it/s][A
                                                 [A 15%|â–ˆâ–Œ        | 15/100 [5:36:27<24:41:45, 1045.95s/it]Average loss of 1.6193745136260986 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 16 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
 13%|â–ˆâ–Ž        | 653/4905 [01:40<10:51,  6.52it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1283/4905 [03:20<09:26,  6.39it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1932/4905 [05:00<07:42,  6.43it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2561/4905 [06:40<06:07,  6.37it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3224/4905 [08:20<04:20,  6.46it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3804/4905 [10:00<02:56,  6.24it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4445/4905 [11:40<01:13,  6.29it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4299131992459297
Last average training loss at batch 1000 out of 4905: 1.3719315603524447
Last average training loss at batch 1500 out of 4905: 1.4234796274602413
Last average training loss at batch 2000 out of 4905: 1.496953121393919
Last average training loss at batch 2500 out of 4905: 1.635081636607647
Last average training loss at batch 3000 out of 4905: 1.733557503581047
Last average training loss at batch 3500 out of 4905: 1.7825421222746372
Last average training loss at batch 4000 out of 4905: 1.8612383000850679
Last average training loss at batch 4500 out of 4905: 1.5286795337200165
Last average training loss at batch 4905 out of 4905: 0.14065002800704993

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:18<?, ?it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 509/702 [01:20<00:30,  6.34it/s][A
                                                 [A 16%|â–ˆâ–Œ        | 16/100 [5:51:05<23:13:35, 995.42s/it] Average loss of 1.626055121421814 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 17 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
 13%|â–ˆâ–Ž        | 644/4905 [01:40<11:02,  6.43it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1266/4905 [03:20<09:37,  6.31it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1877/4905 [05:00<08:07,  6.22it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2496/4905 [06:41<06:29,  6.18it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3150/4905 [08:21<04:38,  6.31it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3727/4905 [10:01<03:12,  6.12it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4345/4905 [11:41<01:31,  6.14it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.427199822485447
Last average training loss at batch 1000 out of 4905: 1.3666259256154298
Last average training loss at batch 1500 out of 4905: 1.4181289462149143
Last average training loss at batch 2000 out of 4905: 1.4918607076406478
Last average training loss at batch 2500 out of 4905: 1.627000919163227
Last average training loss at batch 3000 out of 4905: 1.7247239426076413
Last average training loss at batch 3500 out of 4905: 1.7774932804852723
Last average training loss at batch 4000 out of 4905: 1.8558280892670154
Last average training loss at batch 4500 out of 4905: 1.5289886368513108
Last average training loss at batch 4905 out of 4905: 0.14007533919464923

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 474/702 [01:20<00:38,  5.91it/s][A
                                                 [A 17%|â–ˆâ–‹        | 17/100 [6:06:13<22:20:39, 969.15s/it]Average loss of 1.618512749671936 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 18 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 12%|â–ˆâ–        | 601/4905 [01:40<11:56,  6.00it/s][A
 25%|â–ˆâ–ˆâ–       | 1214/4905 [03:20<10:07,  6.08it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1808/4905 [05:00<08:35,  6.01it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2402/4905 [06:40<06:58,  5.98it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2964/4905 [08:20<05:31,  5.85it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3539/4905 [10:00<03:54,  5.81it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4152/4905 [11:40<02:07,  5.92it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4691/4905 [13:20<00:37,  5.75it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4207339423894882
Last average training loss at batch 1000 out of 4905: 1.364509743452072
Last average training loss at batch 1500 out of 4905: 1.4167732306420804
Last average training loss at batch 2000 out of 4905: 1.4894779291003943
Last average training loss at batch 2500 out of 4905: 1.6222773846238852
Last average training loss at batch 3000 out of 4905: 1.7227335588932038
Last average training loss at batch 3500 out of 4905: 1.7709634717404843
Last average training loss at batch 4000 out of 4905: 1.8540333163440228
Last average training loss at batch 4500 out of 4905: 1.5207994318157434
Last average training loss at batch 4905 out of 4905: 0.1396117777027243

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 460/702 [01:20<00:42,  5.74it/s][A
                                                 [A 18%|â–ˆâ–Š        | 18/100 [6:22:03<21:56:37, 963.39s/it]Average loss of 1.6190167665481567 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 19 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 10%|â–‰         | 485/4905 [01:40<15:19,  4.81it/s][A
 22%|â–ˆâ–ˆâ–       | 1078/4905 [03:20<11:40,  5.46it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1605/4905 [05:01<10:14,  5.37it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2212/4905 [06:41<07:57,  5.65it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2784/4905 [08:21<06:14,  5.66it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3361/4905 [10:01<04:31,  5.70it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3947/4905 [11:41<02:46,  5.75it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4496/4905 [13:21<01:12,  5.67it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4128863639384508
Last average training loss at batch 1000 out of 4905: 1.3596529597342015
Last average training loss at batch 1500 out of 4905: 1.4132879534065723
Last average training loss at batch 2000 out of 4905: 1.4837921987921
Last average training loss at batch 2500 out of 4905: 1.6220305899381637
Last average training loss at batch 3000 out of 4905: 1.7186355286985635
Last average training loss at batch 3500 out of 4905: 1.7654450983256103
Last average training loss at batch 4000 out of 4905: 1.848841107428074
Last average training loss at batch 4500 out of 4905: 1.5143556374907494
Last average training loss at batch 4905 out of 4905: 0.13934153260380241

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 432/702 [01:21<00:50,  5.33it/s][A
                                                 [A 19%|â–ˆâ–‰        | 19/100 [6:38:57<22:01:07, 978.61s/it]Average loss of 1.623914361000061 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 20 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
 10%|â–‰         | 490/4905 [01:40<15:01,  4.90it/s][A
 20%|â–ˆâ–‰        | 973/4905 [03:20<13:29,  4.86it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 1520/4905 [05:00<10:59,  5.14it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1988/4905 [06:40<09:48,  4.96it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2493/4905 [08:20<08:04,  4.98it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3020/4905 [10:00<06:11,  5.08it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3462/4905 [11:40<04:56,  4.86it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3976/4905 [13:21<03:07,  4.95it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4471/4905 [15:01<01:27,  4.95it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4125672771781683
Last average training loss at batch 1000 out of 4905: 1.3545327087193728
Last average training loss at batch 1500 out of 4905: 1.406239427730441
Last average training loss at batch 2000 out of 4905: 1.476587602570653
Last average training loss at batch 2500 out of 4905: 1.6166253917962312
Last average training loss at batch 3000 out of 4905: 1.7108829094469546
Last average training loss at batch 3500 out of 4905: 1.7613131130039692
Last average training loss at batch 4000 out of 4905: 1.8486030432581901
Last average training loss at batch 4500 out of 4905: 1.5113027528375387
Last average training loss at batch 4905 out of 4905: 0.1389840887212

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 335/702 [01:21<01:28,  4.13it/s][A
                                                 [A 20%|â–ˆâ–ˆ        | 20/100 [6:58:11<22:55:06, 1031.33s/it]Average loss of 1.62442147731781 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 21 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
  9%|â–‰         | 435/4905 [01:40<17:07,  4.35it/s][A
 19%|â–ˆâ–‰        | 928/4905 [03:21<14:12,  4.66it/s][A
 28%|â–ˆâ–ˆâ–Š       | 1364/4905 [05:01<13:02,  4.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1755/4905 [06:41<12:15,  4.28it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2211/4905 [08:21<10:15,  4.38it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2617/4905 [10:01<08:55,  4.27it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3044/4905 [11:41<07:15,  4.27it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3485/4905 [13:21<05:29,  4.31it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3943/4905 [15:01<03:38,  4.40it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4337/4905 [16:41<02:13,  4.26it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4767/4905 [18:21<00:32,  4.27it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.4081791975796223
Last average training loss at batch 1000 out of 4905: 1.35276733314991
Last average training loss at batch 1500 out of 4905: 1.402047363743186
Last average training loss at batch 2000 out of 4905: 1.4657855341285466
Last average training loss at batch 2500 out of 4905: 1.5974018230885267
Last average training loss at batch 3000 out of 4905: 1.6953572524785996
Last average training loss at batch 3500 out of 4905: 1.7342796559333802
Last average training loss at batch 4000 out of 4905: 1.820667372226715
Last average training loss at batch 4500 out of 4905: 1.4886511610895394
Last average training loss at batch 4905 out of 4905: 0.13668322324995844

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 320/702 [01:20<01:36,  3.97it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 641/702 [02:41<00:15,  3.98it/s][A
                                                 [A 21%|â–ˆâ–ˆ        | 21/100 [7:20:10<24:31:40, 1117.73s/it]Average loss of 1.6171362400054932 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 22 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
  7%|â–‹         | 345/4905 [01:40<22:02,  3.45it/s][A
 15%|â–ˆâ–        | 723/4905 [03:20<19:07,  3.64it/s][A
 22%|â–ˆâ–ˆâ–       | 1088/4905 [05:00<17:30,  3.63it/s][A
 29%|â–ˆâ–ˆâ–‰       | 1416/4905 [06:40<16:38,  3.49it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1770/4905 [08:21<14:55,  3.50it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2146/4905 [10:01<12:49,  3.59it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2510/4905 [11:41<11:04,  3.60it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2845/4905 [13:22<09:45,  3.52it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3192/4905 [15:02<08:09,  3.50it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3532/4905 [16:42<06:35,  3.47it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3877/4905 [18:26<05:00,  3.42it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4224/4905 [20:06<03:18,  3.43it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4551/4905 [21:47<01:44,  3.37it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4874/4905 [23:28<00:09,  3.33it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.39345831823349
Last average training loss at batch 1000 out of 4905: 1.3363755758851767
Last average training loss at batch 1500 out of 4905: 1.3886055268347264
Last average training loss at batch 2000 out of 4905: 1.4547993820905685
Last average training loss at batch 2500 out of 4905: 1.5864835611581802
Last average training loss at batch 3000 out of 4905: 1.6845578628480435
Last average training loss at batch 3500 out of 4905: 1.7246210067272185
Last average training loss at batch 4000 out of 4905: 1.811090573966503
Last average training loss at batch 4500 out of 4905: 1.4804685321897268
Last average training loss at batch 4905 out of 4905: 0.13624547418438573

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 230/702 [01:20<02:44,  2.87it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 493/702 [02:40<01:07,  3.10it/s][A
                                                 [A 22%|â–ˆâ–ˆâ–       | 22/100 [7:47:45<27:42:41, 1278.99s/it]Average loss of 1.6184097528457642 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 23 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
  5%|â–Œ         | 269/4905 [01:40<28:47,  2.68it/s][A
 12%|â–ˆâ–        | 595/4905 [03:20<23:50,  3.01it/s][A
 18%|â–ˆâ–Š        | 896/4905 [05:01<22:17,  3.00it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 1233/4905 [06:42<19:29,  3.14it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1577/4905 [08:22<17:05,  3.25it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1935/4905 [10:03<14:48,  3.34it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2363/4905 [11:43<11:36,  3.65it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2847/4905 [13:23<08:31,  4.02it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3329/4905 [15:03<06:09,  4.27it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3850/4905 [16:44<03:51,  4.55it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4393/4905 [18:25<01:46,  4.81it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3852258441746235
Last average training loss at batch 1000 out of 4905: 1.3311628195792438
Last average training loss at batch 1500 out of 4905: 1.3812543144375087
Last average training loss at batch 2000 out of 4905: 1.4472019301205874
Last average training loss at batch 2500 out of 4905: 1.5784026054441929
Last average training loss at batch 3000 out of 4905: 1.672618454620242
Last average training loss at batch 3500 out of 4905: 1.7171661049723626
Last average training loss at batch 4000 out of 4905: 1.803922511547804
Last average training loss at batch 4500 out of 4905: 1.4777765050828457
Last average training loss at batch 4905 out of 4905: 0.13586775351099284

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 425/702 [01:20<00:52,  5.31it/s][A
                                                 [A 23%|â–ˆâ–ˆâ–Ž       | 23/100 [8:09:52<27:39:50, 1293.38s/it]Average loss of 1.62179696559906 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 24 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 11%|â–ˆ         | 520/4905 [01:40<14:09,  5.16it/s][A
 21%|â–ˆâ–ˆ        | 1022/4905 [03:20<12:44,  5.08it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 1521/4905 [05:00<11:11,  5.04it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2071/4905 [06:40<09:03,  5.22it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2629/4905 [08:20<07:05,  5.35it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3232/4905 [10:01<05:00,  5.58it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3818/4905 [11:41<03:11,  5.67it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4398/4905 [13:21<01:28,  5.71it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3800552183091641
Last average training loss at batch 1000 out of 4905: 1.3232999877929688
Last average training loss at batch 1500 out of 4905: 1.3736865860968828
Last average training loss at batch 2000 out of 4905: 1.4395106325000524
Last average training loss at batch 2500 out of 4905: 1.5718044532388449
Last average training loss at batch 3000 out of 4905: 1.6693557994961739
Last average training loss at batch 3500 out of 4905: 1.7125091352313757
Last average training loss at batch 4000 out of 4905: 1.798314020216465
Last average training loss at batch 4500 out of 4905: 1.4729725065231323
Last average training loss at batch 4905 out of 4905: 0.13554293103781923

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 403/702 [01:20<00:59,  5.03it/s][A
                                                 [A 24%|â–ˆâ–ˆâ–       | 24/100 [8:27:02<25:38:03, 1214.26s/it]Average loss of 1.61843740940094 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 25 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 11%|â–ˆ         | 529/4905 [01:40<13:48,  5.28it/s][A
 22%|â–ˆâ–ˆâ–       | 1076/4905 [03:20<11:50,  5.39it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1583/4905 [05:00<10:33,  5.24it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2116/4905 [06:40<08:49,  5.27it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2628/4905 [08:20<07:16,  5.21it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3115/4905 [10:00<05:51,  5.10it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3647/4905 [11:40<04:03,  5.17it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4157/4905 [13:21<02:25,  5.14it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4638/4905 [15:01<00:53,  5.04it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.374545906022191
Last average training loss at batch 1000 out of 4905: 1.3170295875817537
Last average training loss at batch 1500 out of 4905: 1.3698754877746104
Last average training loss at batch 2000 out of 4905: 1.4332536780834197
Last average training loss at batch 2500 out of 4905: 1.5672839411199093
Last average training loss at batch 3000 out of 4905: 1.6622096056491136
Last average training loss at batch 3500 out of 4905: 1.7055536741912365
Last average training loss at batch 4000 out of 4905: 1.7930884076356888
Last average training loss at batch 4500 out of 4905: 1.4676231465637684
Last average training loss at batch 4905 out of 4905: 0.1350939148549032

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 382/702 [01:20<01:07,  4.77it/s][A
                                                 [A 25%|â–ˆâ–ˆâ–Œ       | 25/100 [8:45:21<24:34:45, 1179.81s/it]Average loss of 1.6210871934890747 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 26 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
  9%|â–‰         | 451/4905 [01:40<16:27,  4.51it/s][A
 18%|â–ˆâ–Š        | 896/4905 [03:20<14:56,  4.47it/s][A
 28%|â–ˆâ–ˆâ–Š       | 1354/4905 [05:00<13:05,  4.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1790/4905 [06:40<11:39,  4.45it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2311/4905 [08:20<09:09,  4.72it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2806/4905 [10:00<07:17,  4.80it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3283/4905 [11:40<05:38,  4.79it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3870/4905 [13:20<03:21,  5.13it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4329/4905 [15:00<01:56,  4.96it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4845/4905 [16:41<00:11,  5.02it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3696971680670977
Last average training loss at batch 1000 out of 4905: 1.3137419057935475
Last average training loss at batch 1500 out of 4905: 1.3658883026391269
Last average training loss at batch 2000 out of 4905: 1.4290632347464562
Last average training loss at batch 2500 out of 4905: 1.5619765351861716
Last average training loss at batch 3000 out of 4905: 1.6586803055852652
Last average training loss at batch 3500 out of 4905: 1.7013521022796632
Last average training loss at batch 4000 out of 4905: 1.7868018659949303
Last average training loss at batch 4500 out of 4905: 1.462795999020338
Last average training loss at batch 4905 out of 4905: 0.13477487461707885

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:15<?, ?it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 382/702 [01:20<01:07,  4.75it/s][A
                                                 [A 26%|â–ˆâ–ˆâ–Œ       | 26/100 [9:04:46<24:09:22, 1175.17s/it]Average loss of 1.6260676383972168 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 27 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
 10%|â–‰         | 484/4905 [01:40<15:16,  4.82it/s][A
 19%|â–ˆâ–‰        | 949/4905 [03:21<14:03,  4.69it/s][A
 29%|â–ˆâ–ˆâ–‰       | 1431/4905 [05:01<12:12,  4.75it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1920/4905 [06:41<10:21,  4.80it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2328/4905 [08:21<09:27,  4.54it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2806/4905 [10:01<07:34,  4.62it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3281/4905 [11:41<05:48,  4.66it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3640/4905 [13:22<04:53,  4.31it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4115/4905 [15:02<02:57,  4.45it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4587/4905 [16:42<01:10,  4.53it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3673359770476818
Last average training loss at batch 1000 out of 4905: 1.3111966536641122
Last average training loss at batch 1500 out of 4905: 1.3648464729636907
Last average training loss at batch 2000 out of 4905: 1.4275616552084684
Last average training loss at batch 2500 out of 4905: 1.5608089731931687
Last average training loss at batch 3000 out of 4905: 1.659114315509796
Last average training loss at batch 3500 out of 4905: 1.697483864724636
Last average training loss at batch 4000 out of 4905: 1.783676160722971
Last average training loss at batch 4500 out of 4905: 1.4584482777416705
Last average training loss at batch 4905 out of 4905: 0.13440557100964856

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:18<?, ?it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 338/702 [01:22<01:28,  4.10it/s][A
                                                 [A 27%|â–ˆâ–ˆâ–‹       | 27/100 [9:25:12<24:08:33, 1190.60s/it]Average loss of 1.6246254444122314 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 28 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
  8%|â–Š         | 405/4905 [01:40<18:31,  4.05it/s][A
 16%|â–ˆâ–Œ        | 788/4905 [03:20<17:30,  3.92it/s][A
 24%|â–ˆâ–ˆâ–       | 1196/4905 [05:00<15:30,  3.99it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1580/4905 [06:40<14:06,  3.93it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1976/4905 [08:22<12:29,  3.91it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2385/4905 [10:02<10:34,  3.97it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2790/4905 [11:42<08:49,  3.99it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3210/4905 [13:22<06:57,  4.06it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3624/4905 [15:03<05:13,  4.08it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4064/4905 [16:43<03:21,  4.18it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4465/4905 [18:23<01:46,  4.12it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4893/4905 [20:03<00:02,  4.17it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3667213155031204
Last average training loss at batch 1000 out of 4905: 1.3100618652403355
Last average training loss at batch 1500 out of 4905: 1.3618715837597848
Last average training loss at batch 2000 out of 4905: 1.4249290833324193
Last average training loss at batch 2500 out of 4905: 1.559041816353798
Last average training loss at batch 3000 out of 4905: 1.6545383285433053
Last average training loss at batch 3500 out of 4905: 1.6954389405697585
Last average training loss at batch 4000 out of 4905: 1.7803461354076862
Last average training loss at batch 4500 out of 4905: 1.4587316905409098
Last average training loss at batch 4905 out of 4905: 0.13458134725796217

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 328/702 [01:20<01:31,  4.10it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 639/702 [02:40<00:15,  3.97it/s][A
                                                 [A 28%|â–ˆâ–ˆâ–Š       | 28/100 [9:48:20<24:59:47, 1249.82s/it]Average loss of 1.623047947883606 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 29 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
  8%|â–Š         | 393/4905 [01:40<19:09,  3.93it/s][A
 15%|â–ˆâ–Œ        | 737/4905 [03:21<19:14,  3.61it/s][A
 22%|â–ˆâ–ˆâ–       | 1093/4905 [05:01<17:42,  3.59it/s][A
 29%|â–ˆâ–ˆâ–‰       | 1412/4905 [06:41<16:58,  3.43it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 1713/4905 [08:21<16:13,  3.28it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2051/4905 [10:01<14:21,  3.31it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2342/4905 [11:41<13:26,  3.18it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2710/4905 [13:21<10:57,  3.34it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3088/4905 [15:01<08:42,  3.48it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3441/4905 [16:42<06:59,  3.49it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3780/4905 [18:23<05:26,  3.44it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4123/4905 [20:04<03:47,  3.43it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4526/4905 [21:44<01:45,  3.61it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4899/4905 [23:25<00:01,  3.64it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3658366813361644
Last average training loss at batch 1000 out of 4905: 1.3089947182685138
Last average training loss at batch 1500 out of 4905: 1.3601129234284162
Last average training loss at batch 2000 out of 4905: 1.4250455794334411
Last average training loss at batch 2500 out of 4905: 1.5583162326663733
Last average training loss at batch 3000 out of 4905: 1.655157913118601
Last average training loss at batch 3500 out of 4905: 1.6955246921926737
Last average training loss at batch 4000 out of 4905: 1.781709076076746
Last average training loss at batch 4500 out of 4905: 1.4570836418271065
Last average training loss at batch 4905 out of 4905: 0.13437504011313606

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 254/702 [01:20<02:22,  3.15it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 535/702 [02:41<00:49,  3.34it/s][A
                                                 [A 29%|â–ˆâ–ˆâ–‰       | 29/100 [10:15:15<26:48:25, 1359.24s/it]Average loss of 1.6238340139389038 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 30 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
  7%|â–‹         | 319/4905 [01:40<24:04,  3.17it/s][A
 13%|â–ˆâ–Ž        | 647/4905 [03:20<21:57,  3.23it/s][A
 20%|â–ˆâ–‰        | 973/4905 [05:01<20:16,  3.23it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1264/4905 [06:41<19:32,  3.11it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1547/4905 [08:22<18:38,  3.00it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1883/4905 [10:02<16:08,  3.12it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2173/4905 [11:43<14:59,  3.04it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2552/4905 [13:24<12:00,  3.26it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2959/4905 [15:05<09:15,  3.50it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3357/4905 [16:45<07:04,  3.65it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3767/4905 [18:25<05:00,  3.78it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4189/4905 [20:06<03:02,  3.91it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4685/4905 [21:46<00:52,  4.23it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3657919800281524
Last average training loss at batch 1000 out of 4905: 1.3082626647502185
Last average training loss at batch 1500 out of 4905: 1.3604806730747223
Last average training loss at batch 2000 out of 4905: 1.4232776037305594
Last average training loss at batch 2500 out of 4905: 1.556432938158512
Last average training loss at batch 3000 out of 4905: 1.6529498393684625
Last average training loss at batch 3500 out of 4905: 1.6934949931353331
Last average training loss at batch 4000 out of 4905: 1.7803602950870991
Last average training loss at batch 4500 out of 4905: 1.4575139744877814
Last average training loss at batch 4905 out of 4905: 0.13445288732906122

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 390/702 [01:20<01:04,  4.87it/s][A
                                                 [A 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [10:40:07<27:12:21, 1399.16s/it]Average loss of 1.6247899532318115 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 31 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
  9%|â–‰         | 450/4905 [01:40<16:32,  4.49it/s][A
 20%|â–ˆâ–ˆ        | 984/4905 [03:20<13:06,  4.99it/s][A
 29%|â–ˆâ–ˆâ–Š       | 1400/4905 [05:01<12:44,  4.58it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1774/4905 [06:41<12:16,  4.25it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2256/4905 [08:21<09:54,  4.45it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2811/4905 [10:01<07:14,  4.82it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3301/4905 [11:42<05:31,  4.84it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3845/4905 [13:22<03:30,  5.03it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4436/4905 [15:02<01:28,  5.30it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3639208126962186
Last average training loss at batch 1000 out of 4905: 1.3068163581341505
Last average training loss at batch 1500 out of 4905: 1.3589677543491125
Last average training loss at batch 2000 out of 4905: 1.4235532214045525
Last average training loss at batch 2500 out of 4905: 1.554984937041998
Last average training loss at batch 3000 out of 4905: 1.6520749464780091
Last average training loss at batch 3500 out of 4905: 1.6917499312162398
Last average training loss at batch 4000 out of 4905: 1.7798062869310378
Last average training loss at batch 4500 out of 4905: 1.4580480376183986
Last average training loss at batch 4905 out of 4905: 0.13438307397318178

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 424/702 [01:20<00:52,  5.30it/s][A
                                                 [A 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [10:58:43<25:11:13, 1314.10s/it]Average loss of 1.6249849796295166 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 32 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 11%|â–ˆ         | 539/4905 [01:40<13:30,  5.39it/s][A
 22%|â–ˆâ–ˆâ–       | 1068/4905 [03:20<12:00,  5.33it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1617/4905 [05:00<10:09,  5.40it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2023/4905 [06:40<09:52,  4.87it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2351/4905 [08:20<09:54,  4.30it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2696/4905 [10:00<09:11,  4.01it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3237/4905 [11:40<06:13,  4.46it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3757/4905 [13:20<04:04,  4.69it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4264/4905 [15:01<02:13,  4.80it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4804/4905 [16:42<00:20,  4.98it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3640529403835535
Last average training loss at batch 1000 out of 4905: 1.3069279157072307
Last average training loss at batch 1500 out of 4905: 1.3574415892213583
Last average training loss at batch 2000 out of 4905: 1.4219801902174949
Last average training loss at batch 2500 out of 4905: 1.5535752671509981
Last average training loss at batch 3000 out of 4905: 1.6521348397433757
Last average training loss at batch 3500 out of 4905: 1.6921383808255195
Last average training loss at batch 4000 out of 4905: 1.7772872754782438
Last average training loss at batch 4500 out of 4905: 1.4574966348409653
Last average training loss at batch 4905 out of 4905: 0.13439292524628926

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 384/702 [01:20<01:06,  4.80it/s][A
                                                 [A 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [11:18:12<24:00:11, 1270.75s/it]Average loss of 1.625586986541748 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 33 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 10%|â–‰         | 487/4905 [01:40<15:08,  4.87it/s][A
 20%|â–ˆâ–ˆ        | 988/4905 [03:20<13:11,  4.95it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 1493/4905 [05:00<11:23,  4.99it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1932/4905 [06:42<10:30,  4.72it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2393/4905 [08:22<08:56,  4.68it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2861/4905 [10:02<07:17,  4.68it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3348/4905 [11:42<05:28,  4.74it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3820/4905 [13:22<03:49,  4.73it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4338/4905 [15:02<01:56,  4.87it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4758/4905 [16:42<00:31,  4.66it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3632027775347233
Last average training loss at batch 1000 out of 4905: 1.306498564004898
Last average training loss at batch 1500 out of 4905: 1.3604361901134252
Last average training loss at batch 2000 out of 4905: 1.4224188245981932
Last average training loss at batch 2500 out of 4905: 1.555188111707568
Last average training loss at batch 3000 out of 4905: 1.6515493089556694
Last average training loss at batch 3500 out of 4905: 1.689760115712881
Last average training loss at batch 4000 out of 4905: 1.7781926217675208
Last average training loss at batch 4500 out of 4905: 1.4557345827817918
Last average training loss at batch 4905 out of 4905: 0.13416144509441869

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 321/702 [01:20<01:36,  3.96it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 697/702 [02:41<00:01,  4.38it/s][A
                                                 [A 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [11:38:09<23:14:10, 1248.52s/it]Average loss of 1.6232830286026 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 34 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
  8%|â–Š         | 383/4905 [01:40<19:42,  3.82it/s][A
 16%|â–ˆâ–‹        | 806/4905 [03:20<16:48,  4.06it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1253/4905 [05:00<14:19,  4.25it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 1690/4905 [06:40<12:28,  4.30it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2138/4905 [08:21<10:36,  4.35it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2609/4905 [10:01<08:33,  4.47it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3012/4905 [11:41<07:17,  4.32it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3387/4905 [13:21<06:06,  4.14it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3769/4905 [15:02<04:41,  4.03it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4148/4905 [16:42<03:11,  3.96it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4655/4905 [18:22<00:58,  4.29it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.364021799877286
Last average training loss at batch 1000 out of 4905: 1.306052164748311
Last average training loss at batch 1500 out of 4905: 1.3586762854903935
Last average training loss at batch 2000 out of 4905: 1.4214915680736304
Last average training loss at batch 2500 out of 4905: 1.5530481727719307
Last average training loss at batch 3000 out of 4905: 1.6518329100608826
Last average training loss at batch 3500 out of 4905: 1.689573987364769
Last average training loss at batch 4000 out of 4905: 1.7780866546332836
Last average training loss at batch 4500 out of 4905: 1.4555903087109328
Last average training loss at batch 4905 out of 4905: 0.13417748549875255

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 323/702 [01:20<01:33,  4.04it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 615/702 [02:41<00:23,  3.77it/s][A
                                                 [A 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [12:00:33<23:25:03, 1277.33s/it]Average loss of 1.6243551969528198 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 35 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
  8%|â–Š         | 379/4905 [01:40<19:55,  3.78it/s][A
 17%|â–ˆâ–‹        | 828/4905 [03:20<16:11,  4.20it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1257/4905 [05:00<14:21,  4.24it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1645/4905 [06:40<13:16,  4.09it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2070/4905 [08:20<11:23,  4.15it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2473/4905 [10:00<09:52,  4.10it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2910/4905 [11:41<07:56,  4.19it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3307/4905 [13:21<06:28,  4.12it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3718/4905 [15:13<04:59,  3.97it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4127/4905 [16:53<03:14,  4.00it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4504/4905 [18:34<01:42,  3.92it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4895/4905 [20:14<00:02,  3.92it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3611323435753584
Last average training loss at batch 1000 out of 4905: 1.307089120939374
Last average training loss at batch 1500 out of 4905: 1.3588120828866959
Last average training loss at batch 2000 out of 4905: 1.4211171502768993
Last average training loss at batch 2500 out of 4905: 1.552805999726057
Last average training loss at batch 3000 out of 4905: 1.652251616552472
Last average training loss at batch 3500 out of 4905: 1.693872688382864
Last average training loss at batch 4000 out of 4905: 1.7771831738650798
Last average training loss at batch 4500 out of 4905: 1.4554527404755353
Last average training loss at batch 4905 out of 4905: 0.13422875790937475

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 284/702 [01:20<01:58,  3.54it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 550/702 [02:40<00:44,  3.40it/s][A
                                                 [A 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [12:24:09<23:48:39, 1318.76s/it]Average loss of 1.6245629787445068 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 36 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
  7%|â–‹         | 357/4905 [01:41<21:35,  3.51it/s][A
 14%|â–ˆâ–        | 701/4905 [03:22<20:14,  3.46it/s][A
 21%|â–ˆâ–ˆâ–       | 1046/4905 [05:02<18:39,  3.45it/s][A
 30%|â–ˆâ–ˆâ–‰       | 1460/4905 [06:42<15:26,  3.72it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1859/4905 [08:22<13:18,  3.82it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2236/4905 [10:03<11:44,  3.79it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2592/4905 [11:43<10:23,  3.71it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3004/4905 [13:23<08:15,  3.84it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3358/4905 [15:03<06:52,  3.75it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3743/4905 [16:44<05:07,  3.78it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4134/4905 [18:24<03:22,  3.82it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4612/4905 [20:04<01:11,  4.11it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362515343144536
Last average training loss at batch 1000 out of 4905: 1.3064270753264426
Last average training loss at batch 1500 out of 4905: 1.3579441590458154
Last average training loss at batch 2000 out of 4905: 1.4201735870093106
Last average training loss at batch 2500 out of 4905: 1.5538684444725512
Last average training loss at batch 3000 out of 4905: 1.6504526239931583
Last average training loss at batch 3500 out of 4905: 1.6906106932759284
Last average training loss at batch 4000 out of 4905: 1.7781366589665413
Last average training loss at batch 4500 out of 4905: 1.4570850918591023
Last average training loss at batch 4905 out of 4905: 0.1342281332135079

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 385/702 [01:20<01:06,  4.80it/s][A
                                                 [A 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [12:47:46<23:57:59, 1348.12s/it]Average loss of 1.627070665359497 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 37 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
 10%|â–‰         | 482/4905 [01:40<15:18,  4.82it/s][A
 20%|â–ˆâ–‰        | 971/4905 [03:20<13:29,  4.86it/s][A
 29%|â–ˆâ–ˆâ–‰       | 1433/4905 [05:00<12:12,  4.74it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1907/4905 [06:40<10:33,  4.74it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2442/4905 [08:20<08:17,  4.96it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2996/4905 [10:00<06:10,  5.15it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3560/4905 [11:41<04:13,  5.30it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4038/4905 [13:21<02:48,  5.14it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4627/4905 [15:01<00:51,  5.37it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3633232332915068
Last average training loss at batch 1000 out of 4905: 1.307090103790164
Last average training loss at batch 1500 out of 4905: 1.3588275090754032
Last average training loss at batch 2000 out of 4905: 1.4214188165664672
Last average training loss at batch 2500 out of 4905: 1.5532682886868716
Last average training loss at batch 3000 out of 4905: 1.6508299496918917
Last average training loss at batch 3500 out of 4905: 1.6892733744829893
Last average training loss at batch 4000 out of 4905: 1.7779134387671947
Last average training loss at batch 4500 out of 4905: 1.4562865765839814
Last average training loss at batch 4905 out of 4905: 0.1341586038485458

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 377/702 [01:20<01:08,  4.71it/s][A
                                                 [A 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [13:05:49<22:12:09, 1268.73s/it]Average loss of 1.6250730752944946 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 38 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
 11%|â–ˆ         | 536/4905 [01:40<13:35,  5.36it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1151/4905 [03:20<10:44,  5.82it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1759/4905 [05:00<08:49,  5.94it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2385/4905 [06:40<06:56,  6.05it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2978/4905 [08:20<05:20,  6.01it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3579/4905 [10:00<03:40,  6.01it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4223/4905 [11:40<01:50,  6.15it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4834/4905 [13:20<00:11,  6.14it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3618680633157492
Last average training loss at batch 1000 out of 4905: 1.306373455375433
Last average training loss at batch 1500 out of 4905: 1.3572026634514331
Last average training loss at batch 2000 out of 4905: 1.4218393510580063
Last average training loss at batch 2500 out of 4905: 1.5537051910758017
Last average training loss at batch 3000 out of 4905: 1.6501736547499894
Last average training loss at batch 3500 out of 4905: 1.6905056058466434
Last average training loss at batch 4000 out of 4905: 1.7772950785160064
Last average training loss at batch 4500 out of 4905: 1.4557887595891952
Last average training loss at batch 4905 out of 4905: 0.1342396769015675

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 474/702 [01:20<00:38,  5.92it/s][A
                                                 [A 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [13:21:23<20:07:14, 1168.29s/it]Average loss of 1.6246014833450317 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 39 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 10%|â–‰         | 487/4905 [01:40<15:08,  4.86it/s][A
 21%|â–ˆâ–ˆâ–       | 1050/4905 [03:20<12:05,  5.31it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1618/4905 [05:00<09:59,  5.48it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2258/4905 [06:40<07:33,  5.84it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2900/4905 [08:20<05:31,  6.05it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3519/4905 [10:00<03:47,  6.10it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4143/4905 [11:40<02:04,  6.14it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4775/4905 [13:20<00:20,  6.20it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3632773917317391
Last average training loss at batch 1000 out of 4905: 1.3050235689431429
Last average training loss at batch 1500 out of 4905: 1.3585765989869834
Last average training loss at batch 2000 out of 4905: 1.422334019869566
Last average training loss at batch 2500 out of 4905: 1.5529780881553887
Last average training loss at batch 3000 out of 4905: 1.6504378287047148
Last average training loss at batch 3500 out of 4905: 1.691755915388465
Last average training loss at batch 4000 out of 4905: 1.777496482938528
Last average training loss at batch 4500 out of 4905: 1.455808644592762
Last average training loss at batch 4905 out of 4905: 0.1342585505103117

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 453/702 [01:20<00:44,  5.63it/s][A
                                                 [A 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [13:37:11<18:40:36, 1102.24s/it]Average loss of 1.6227383613586426 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 40 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
 12%|â–ˆâ–        | 582/4905 [01:40<12:23,  5.81it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1114/4905 [03:20<11:26,  5.52it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 1670/4905 [05:00<09:44,  5.53it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2200/4905 [06:40<08:18,  5.43it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2839/4905 [08:21<05:57,  5.77it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3429/4905 [10:01<04:13,  5.82it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4048/4905 [11:41<02:24,  5.94it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4725/4905 [13:21<00:29,  6.20it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.361157262444496
Last average training loss at batch 1000 out of 4905: 1.306016100510955
Last average training loss at batch 1500 out of 4905: 1.357687539190054
Last average training loss at batch 2000 out of 4905: 1.4207631887346506
Last average training loss at batch 2500 out of 4905: 1.5528000311553478
Last average training loss at batch 3000 out of 4905: 1.6505072122216224
Last average training loss at batch 3500 out of 4905: 1.6913046129494906
Last average training loss at batch 4000 out of 4905: 1.7770049058794974
Last average training loss at batch 4500 out of 4905: 1.4562148749530315
Last average training loss at batch 4905 out of 4905: 0.13426406378046088

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 425/702 [01:20<00:52,  5.29it/s][A
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [13:53:12<17:39:53, 1059.90s/it]Average loss of 1.6254616975784302 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 41 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 10%|â–ˆ         | 504/4905 [01:40<14:33,  5.04it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1107/4905 [03:20<11:16,  5.62it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 1657/4905 [05:00<09:43,  5.56it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2255/4905 [06:40<07:42,  5.73it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2792/4905 [08:20<06:17,  5.59it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3347/4905 [10:00<04:39,  5.57it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3859/4905 [11:40<03:12,  5.42it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4424/4905 [13:20<01:27,  5.50it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3624372012764216
Last average training loss at batch 1000 out of 4905: 1.305544365182519
Last average training loss at batch 1500 out of 4905: 1.3582295053601265
Last average training loss at batch 2000 out of 4905: 1.4205146580040455
Last average training loss at batch 2500 out of 4905: 1.551818052276969
Last average training loss at batch 3000 out of 4905: 1.6514194838702678
Last average training loss at batch 3500 out of 4905: 1.690786282196641
Last average training loss at batch 4000 out of 4905: 1.7780777108371257
Last average training loss at batch 4500 out of 4905: 1.4550670219212771
Last average training loss at batch 4905 out of 4905: 0.1341886747032252

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:18<?, ?it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 416/702 [01:20<00:55,  5.19it/s][A
                                                 [A 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [14:10:02<17:07:28, 1044.89s/it]Average loss of 1.6275949478149414 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 42 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 12%|â–ˆâ–        | 567/4905 [01:40<12:45,  5.67it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 1155/4905 [03:20<10:47,  5.79it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1732/4905 [05:02<09:13,  5.73it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2290/4905 [06:42<07:41,  5.66it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2795/4905 [08:22<06:27,  5.44it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3278/4905 [10:02<05:10,  5.23it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3712/4905 [11:42<04:01,  4.94it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4232/4905 [13:23<02:14,  5.01it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4742/4905 [15:03<00:32,  5.04it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362170015156269
Last average training loss at batch 1000 out of 4905: 1.306896313264966
Last average training loss at batch 1500 out of 4905: 1.3583520206958055
Last average training loss at batch 2000 out of 4905: 1.4207922445833683
Last average training loss at batch 2500 out of 4905: 1.552421203672886
Last average training loss at batch 3000 out of 4905: 1.6505449071377516
Last average training loss at batch 3500 out of 4905: 1.6890144901573658
Last average training loss at batch 4000 out of 4905: 1.7777734441161155
Last average training loss at batch 4500 out of 4905: 1.4561886761039495
Last average training loss at batch 4905 out of 4905: 0.13412463603186195

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 357/702 [01:20<01:17,  4.45it/s][A
                                                 [A 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [14:28:20<17:05:24, 1060.76s/it]Average loss of 1.6247937679290771 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 43 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
  8%|â–Š         | 413/4905 [01:40<18:08,  4.13it/s][A
 18%|â–ˆâ–Š        | 889/4905 [03:20<14:53,  4.50it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1335/4905 [05:00<13:17,  4.48it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1807/4905 [06:40<11:18,  4.57it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2256/4905 [08:20<09:43,  4.54it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2784/4905 [10:00<07:22,  4.79it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3275/4905 [11:41<05:38,  4.82it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3813/4905 [13:21<03:38,  4.99it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4344/4905 [15:02<01:50,  5.08it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4868/4905 [16:42<00:07,  5.13it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.360761646181345
Last average training loss at batch 1000 out of 4905: 1.3058131137788296
Last average training loss at batch 1500 out of 4905: 1.3595582026988269
Last average training loss at batch 2000 out of 4905: 1.420555226266384
Last average training loss at batch 2500 out of 4905: 1.5524710594415665
Last average training loss at batch 3000 out of 4905: 1.649627669274807
Last average training loss at batch 3500 out of 4905: 1.690849157705903
Last average training loss at batch 4000 out of 4905: 1.777362455844879
Last average training loss at batch 4500 out of 4905: 1.455557089790702
Last average training loss at batch 4905 out of 4905: 0.13416455148070838

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 380/702 [01:20<01:07,  4.75it/s][A
                                                 [A 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [14:47:34<17:14:23, 1088.83s/it]Average loss of 1.6232680082321167 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 44 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 11%|â–ˆ         | 522/4905 [01:40<13:59,  5.22it/s][A
 22%|â–ˆâ–ˆâ–       | 1089/4905 [03:20<11:35,  5.48it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1652/4905 [05:00<09:46,  5.55it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2216/4905 [06:40<08:01,  5.58it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2737/4905 [08:20<06:37,  5.45it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3062/4905 [10:00<06:32,  4.69it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3423/4905 [11:40<05:41,  4.34it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3765/4905 [13:21<04:42,  4.04it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4235/4905 [15:01<02:37,  4.24it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4806/4905 [16:41<00:21,  4.70it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362419244363904
Last average training loss at batch 1000 out of 4905: 1.3063767509758473
Last average training loss at batch 1500 out of 4905: 1.3578242383748294
Last average training loss at batch 2000 out of 4905: 1.4216250948756932
Last average training loss at batch 2500 out of 4905: 1.552953786969185
Last average training loss at batch 3000 out of 4905: 1.6513916949480771
Last average training loss at batch 3500 out of 4905: 1.6907462707161904
Last average training loss at batch 4000 out of 4905: 1.7789447798132896
Last average training loss at batch 4500 out of 4905: 1.4555373299121857
Last average training loss at batch 4905 out of 4905: 0.13431811186880876

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 461/702 [01:20<00:42,  5.74it/s][A
                                                 [A 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [15:06:38<17:11:33, 1105.25s/it]Average loss of 1.623698115348816 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 45 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:18<?, ?it/s][A
 11%|â–ˆ         | 541/4905 [01:40<13:29,  5.39it/s][A
 18%|â–ˆâ–Š        | 861/4905 [03:20<16:25,  4.10it/s][A
 25%|â–ˆâ–ˆâ–       | 1212/4905 [05:00<16:03,  3.83it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 1574/4905 [06:40<14:49,  3.74it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1945/4905 [08:20<13:13,  3.73it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2517/4905 [10:00<09:02,  4.40it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3046/4905 [11:41<06:36,  4.69it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3648/4905 [13:21<04:05,  5.11it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4178/4905 [15:01<02:20,  5.17it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4709/4905 [16:41<00:37,  5.21it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3610374207496643
Last average training loss at batch 1000 out of 4905: 1.3066868313401938
Last average training loss at batch 1500 out of 4905: 1.3567468927651645
Last average training loss at batch 2000 out of 4905: 1.4198876851201057
Last average training loss at batch 2500 out of 4905: 1.5543087870180607
Last average training loss at batch 3000 out of 4905: 1.6506300381720067
Last average training loss at batch 3500 out of 4905: 1.690664414614439
Last average training loss at batch 4000 out of 4905: 1.777188481837511
Last average training loss at batch 4500 out of 4905: 1.4575839086323976
Last average training loss at batch 4905 out of 4905: 0.1342297251922032

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 467/702 [01:20<00:40,  5.82it/s][A
                                                 [A 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [15:25:54<17:07:15, 1120.64s/it]Average loss of 1.6262898445129395 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 46 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 11%|â–ˆâ–        | 564/4905 [01:40<12:50,  5.63it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1120/4905 [03:20<11:17,  5.59it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1735/4905 [05:00<09:02,  5.84it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2316/4905 [06:40<07:24,  5.83it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2875/4905 [08:20<05:53,  5.74it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3410/4905 [10:00<04:26,  5.61it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3991/4905 [11:40<02:41,  5.67it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4478/4905 [13:20<01:18,  5.41it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3616221254915
Last average training loss at batch 1000 out of 4905: 1.3060590007901192
Last average training loss at batch 1500 out of 4905: 1.3575749018639327
Last average training loss at batch 2000 out of 4905: 1.4202665397375822
Last average training loss at batch 2500 out of 4905: 1.5528779289871455
Last average training loss at batch 3000 out of 4905: 1.650668283313513
Last average training loss at batch 3500 out of 4905: 1.6894392304122448
Last average training loss at batch 4000 out of 4905: 1.77702281370759
Last average training loss at batch 4500 out of 4905: 1.4559843940883874
Last average training loss at batch 4905 out of 4905: 0.13423154949838112

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 460/702 [01:20<00:42,  5.73it/s][A
                                                 [A 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [15:42:30<16:14:49, 1083.14s/it]Average loss of 1.6253241300582886 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 47 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
 12%|â–ˆâ–        | 580/4905 [01:40<12:26,  5.79it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1146/4905 [03:20<10:57,  5.71it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 1691/4905 [05:00<09:34,  5.59it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2231/4905 [06:40<08:04,  5.52it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2817/4905 [08:20<06:10,  5.64it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3411/4905 [10:00<04:20,  5.74it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4005/4905 [11:40<02:35,  5.80it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4578/4905 [13:21<00:56,  5.77it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3615551537424326
Last average training loss at batch 1000 out of 4905: 1.30651260407269
Last average training loss at batch 1500 out of 4905: 1.3571416731774808
Last average training loss at batch 2000 out of 4905: 1.4216682987958194
Last average training loss at batch 2500 out of 4905: 1.5525480280667543
Last average training loss at batch 3000 out of 4905: 1.6494219591617585
Last average training loss at batch 3500 out of 4905: 1.6901801876872777
Last average training loss at batch 4000 out of 4905: 1.7773681981563567
Last average training loss at batch 4500 out of 4905: 1.456373012304306
Last average training loss at batch 4905 out of 4905: 0.13435244818315836

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 467/702 [01:20<00:40,  5.83it/s][A
                                                 [A 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [15:58:49<15:29:17, 1052.03s/it]Average loss of 1.627206802368164 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 48 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
 12%|â–ˆâ–        | 566/4905 [01:40<12:50,  5.63it/s][A
 22%|â–ˆâ–ˆâ–       | 1096/4905 [03:20<11:40,  5.44it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1621/4905 [05:00<10:13,  5.35it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2151/4905 [06:40<08:36,  5.33it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2550/4905 [08:21<08:06,  4.84it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2936/4905 [10:02<07:18,  4.49it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3284/4905 [11:42<06:29,  4.16it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3682/4905 [13:23<04:59,  4.08it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4165/4905 [15:03<02:51,  4.31it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4747/4905 [16:43<00:33,  4.78it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3618832387179136
Last average training loss at batch 1000 out of 4905: 1.305945388764143
Last average training loss at batch 1500 out of 4905: 1.3584269021749495
Last average training loss at batch 2000 out of 4905: 1.419700613901019
Last average training loss at batch 2500 out of 4905: 1.5528478642106056
Last average training loss at batch 3000 out of 4905: 1.6494753573536873
Last average training loss at batch 3500 out of 4905: 1.6913978641480207
Last average training loss at batch 4000 out of 4905: 1.7772481497228145
Last average training loss at batch 4500 out of 4905: 1.4564613967835902
Last average training loss at batch 4905 out of 4905: 0.13417987376907187

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 333/702 [01:20<01:29,  4.13it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 645/702 [02:40<00:14,  3.99it/s][A
                                                 [A 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [16:19:03<15:53:44, 1100.47s/it]Average loss of 1.624424934387207 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 49 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 10%|â–‰         | 469/4905 [01:40<15:46,  4.69it/s][A
 20%|â–ˆâ–‰        | 978/4905 [03:20<13:18,  4.92it/s][A
 30%|â–ˆâ–ˆâ–‰       | 1466/4905 [05:00<11:41,  4.90it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1993/4905 [06:40<09:37,  5.04it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2527/4905 [08:20<07:42,  5.15it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3032/4905 [10:01<06:07,  5.10it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3425/4905 [11:41<05:13,  4.72it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3758/4905 [13:21<04:28,  4.28it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4063/4905 [15:03<03:37,  3.87it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4278/4905 [16:43<03:07,  3.34it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4593/4905 [18:24<01:35,  3.28it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4887/4905 [20:04<00:05,  3.17it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362493258923292
Last average training loss at batch 1000 out of 4905: 1.3074497239887715
Last average training loss at batch 1500 out of 4905: 1.3578805032074452
Last average training loss at batch 2000 out of 4905: 1.4201408890783787
Last average training loss at batch 2500 out of 4905: 1.5514531553536655
Last average training loss at batch 3000 out of 4905: 1.6501368468403816
Last average training loss at batch 3500 out of 4905: 1.6899808772951364
Last average training loss at batch 4000 out of 4905: 1.7783659915924073
Last average training loss at batch 4500 out of 4905: 1.4559899187535048
Last average training loss at batch 4905 out of 4905: 0.13422195268272746

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 274/702 [01:20<02:05,  3.42it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 592/702 [02:41<00:29,  3.71it/s][A
                                                 [A 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [16:42:17<16:50:19, 1188.61s/it]Average loss of 1.624177098274231 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 50 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:18<?, ?it/s][A
 11%|â–ˆ         | 523/4905 [01:40<14:00,  5.22it/s][A
 21%|â–ˆâ–ˆ        | 1010/4905 [03:20<12:57,  5.01it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 1486/4905 [05:00<11:39,  4.89it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2045/4905 [06:40<09:13,  5.16it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2607/4905 [08:20<07:11,  5.33it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3159/4905 [10:00<05:23,  5.39it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3713/4905 [11:41<03:39,  5.42it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4296/4905 [13:21<01:49,  5.55it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4840/4905 [15:01<00:11,  5.52it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3614355855584144
Last average training loss at batch 1000 out of 4905: 1.304695678666234
Last average training loss at batch 1500 out of 4905: 1.3573817134350539
Last average training loss at batch 2000 out of 4905: 1.4197487961500883
Last average training loss at batch 2500 out of 4905: 1.552790540367365
Last average training loss at batch 3000 out of 4905: 1.6495591630935669
Last average training loss at batch 3500 out of 4905: 1.6911560332626103
Last average training loss at batch 4000 out of 4905: 1.7762553973793984
Last average training loss at batch 4500 out of 4905: 1.4556047810316086
Last average training loss at batch 4905 out of 4905: 0.1342051511875351

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 488/702 [01:20<00:35,  6.05it/s][A
                                                 [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [16:59:35<15:52:49, 1143.40s/it]Average loss of 1.6227370500564575 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 51 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 11%|â–ˆ         | 541/4905 [01:40<13:27,  5.41it/s][A
 21%|â–ˆâ–ˆâ–       | 1045/4905 [03:20<12:24,  5.19it/s][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 1541/4905 [05:00<11:02,  5.08it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2056/4905 [06:40<09:18,  5.10it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2614/4905 [08:20<07:14,  5.27it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2949/4905 [10:00<07:03,  4.62it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3154/4905 [11:41<07:43,  3.77it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3695/4905 [13:21<04:41,  4.29it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4256/4905 [15:01<02:17,  4.70it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4820/4905 [16:41<00:17,  4.99it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3626491972208024
Last average training loss at batch 1000 out of 4905: 1.306612351372838
Last average training loss at batch 1500 out of 4905: 1.3578547519743442
Last average training loss at batch 2000 out of 4905: 1.4212496642023325
Last average training loss at batch 2500 out of 4905: 1.552903877094388
Last average training loss at batch 3000 out of 4905: 1.6504863005280495
Last average training loss at batch 3500 out of 4905: 1.6916866138279438
Last average training loss at batch 4000 out of 4905: 1.775899126306176
Last average training loss at batch 4500 out of 4905: 1.4541967453807592
Last average training loss at batch 4905 out of 4905: 0.13420531666029728

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 497/702 [01:20<00:33,  6.15it/s][A
                                                 [A 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [17:18:32<15:32:16, 1141.55s/it]Average loss of 1.6241642236709595 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 52 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 11%|â–ˆâ–        | 552/4905 [01:40<13:08,  5.52it/s][A
 22%|â–ˆâ–ˆâ–       | 1101/4905 [03:20<11:31,  5.50it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 1697/4905 [05:00<09:22,  5.71it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2266/4905 [06:40<07:43,  5.70it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2823/4905 [08:23<06:13,  5.58it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3432/4905 [10:03<04:16,  5.75it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4071/4905 [11:44<02:20,  5.96it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4725/4905 [13:24<00:29,  6.14it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3625543093383312
Last average training loss at batch 1000 out of 4905: 1.3060183163285255
Last average training loss at batch 1500 out of 4905: 1.3573326660841702
Last average training loss at batch 2000 out of 4905: 1.4227611749470235
Last average training loss at batch 2500 out of 4905: 1.5531793772131204
Last average training loss at batch 3000 out of 4905: 1.650459778815508
Last average training loss at batch 3500 out of 4905: 1.6917386621683836
Last average training loss at batch 4000 out of 4905: 1.778217903792858
Last average training loss at batch 4500 out of 4905: 1.455653451398015
Last average training loss at batch 4905 out of 4905: 0.13422061020202714

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 534/702 [01:20<00:25,  6.67it/s][A
                                                 [A 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [17:34:11<14:24:35, 1080.74s/it]Average loss of 1.6252176761627197 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 53 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
 13%|â–ˆâ–Ž        | 618/4905 [01:40<11:34,  6.17it/s][A
 25%|â–ˆâ–ˆâ–       | 1207/4905 [03:20<10:16,  6.00it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1744/4905 [05:02<09:17,  5.67it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2098/4905 [06:43<09:43,  4.81it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2345/4905 [08:24<10:46,  3.96it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2853/4905 [10:04<07:53,  4.33it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2971/4905 [11:58<10:11,  3.16it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3097/4905 [13:38<11:41,  2.58it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3434/4905 [15:18<08:41,  2.82it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4077/4905 [16:58<03:31,  3.91it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4724/4905 [18:38<00:38,  4.69it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3622316409647466
Last average training loss at batch 1000 out of 4905: 1.305880451992154
Last average training loss at batch 1500 out of 4905: 1.3572525481432676
Last average training loss at batch 2000 out of 4905: 1.4204345815181731
Last average training loss at batch 2500 out of 4905: 1.5529044188559056
Last average training loss at batch 3000 out of 4905: 1.6499739420711994
Last average training loss at batch 3500 out of 4905: 1.6905780681669713
Last average training loss at batch 4000 out of 4905: 1.7773421512842178
Last average training loss at batch 4500 out of 4905: 1.4562514183819295
Last average training loss at batch 4905 out of 4905: 0.1342695271871627

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:15<?, ?it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 497/702 [01:20<00:33,  6.21it/s][A
                                                 [A 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [17:55:11<14:48:47, 1134.63s/it]Average loss of 1.6251013278961182 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 54 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
 13%|â–ˆâ–Ž        | 652/4905 [01:40<10:52,  6.52it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 1232/4905 [03:20<10:02,  6.09it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1806/4905 [05:00<08:42,  5.93it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2455/4905 [06:40<06:38,  6.15it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3090/4905 [08:20<04:51,  6.22it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3723/4905 [10:01<03:09,  6.23it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4278/4905 [11:41<01:44,  6.01it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4872/4905 [13:21<00:05,  5.99it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3607932091355324
Last average training loss at batch 1000 out of 4905: 1.3054689213782549
Last average training loss at batch 1500 out of 4905: 1.3586861356943847
Last average training loss at batch 2000 out of 4905: 1.4203069236576558
Last average training loss at batch 2500 out of 4905: 1.5524579873234035
Last average training loss at batch 3000 out of 4905: 1.6507239450067281
Last average training loss at batch 3500 out of 4905: 1.689463617399335
Last average training loss at batch 4000 out of 4905: 1.777401514157653
Last average training loss at batch 4500 out of 4905: 1.454846317693591
Last average training loss at batch 4905 out of 4905: 0.13413152191617073

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 536/702 [01:20<00:24,  6.69it/s][A
                                                 [A 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [18:10:23<13:38:34, 1067.70s/it]Average loss of 1.6246687173843384 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 55 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 13%|â–ˆâ–Ž        | 615/4905 [01:40<11:37,  6.15it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1263/4905 [03:20<09:34,  6.34it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1891/4905 [05:00<07:57,  6.31it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2548/4905 [06:40<06:07,  6.41it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3213/4905 [08:20<04:20,  6.49it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3723/4905 [10:00<03:16,  6.02it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4200/4905 [11:40<02:05,  5.61it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4640/4905 [13:20<00:50,  5.22it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3615115759521723
Last average training loss at batch 1000 out of 4905: 1.3067711573690175
Last average training loss at batch 1500 out of 4905: 1.3571169132441283
Last average training loss at batch 2000 out of 4905: 1.4200487831383943
Last average training loss at batch 2500 out of 4905: 1.552816760390997
Last average training loss at batch 3000 out of 4905: 1.6500356915593146
Last average training loss at batch 3500 out of 4905: 1.6905414083898067
Last average training loss at batch 4000 out of 4905: 1.7775778888463973
Last average training loss at batch 4500 out of 4905: 1.4555735445022584
Last average training loss at batch 4905 out of 4905: 0.1343017066032393

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 340/702 [01:20<01:25,  4.24it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 629/702 [02:40<00:18,  3.86it/s][A
                                                 [A 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [18:27:32<13:12:09, 1056.22s/it]Average loss of 1.6245315074920654 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 56 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 12%|â–ˆâ–        | 575/4905 [01:40<12:33,  5.75it/s][A
 24%|â–ˆâ–ˆâ–       | 1172/4905 [03:20<10:35,  5.88it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1781/4905 [05:00<08:42,  5.97it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2374/4905 [06:40<07:05,  5.95it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2992/4905 [08:20<05:17,  6.03it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3624/4905 [10:00<03:29,  6.13it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4199/4905 [11:40<01:57,  6.00it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4803/4905 [13:20<00:16,  6.01it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3615976220220327
Last average training loss at batch 1000 out of 4905: 1.305518185377121
Last average training loss at batch 1500 out of 4905: 1.35813314409554
Last average training loss at batch 2000 out of 4905: 1.421335172995925
Last average training loss at batch 2500 out of 4905: 1.5535486162900924
Last average training loss at batch 3000 out of 4905: 1.6498727779388427
Last average training loss at batch 3500 out of 4905: 1.6923245822638273
Last average training loss at batch 4000 out of 4905: 1.778180049151182
Last average training loss at batch 4500 out of 4905: 1.4560227160602808
Last average training loss at batch 4905 out of 4905: 0.13425823477126286

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:15<?, ?it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 448/702 [01:21<00:46,  5.46it/s][A
                                                 [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [18:43:44<12:35:57, 1030.86s/it]Average loss of 1.62199068069458 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 57 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
  8%|â–Š         | 380/4905 [01:40<19:51,  3.80it/s][A
 16%|â–ˆâ–Œ        | 762/4905 [03:20<18:07,  3.81it/s][A
 22%|â–ˆâ–ˆâ–       | 1073/4905 [05:00<18:18,  3.49it/s][A
 29%|â–ˆâ–ˆâ–‰       | 1421/4905 [06:40<16:39,  3.48it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1858/4905 [08:20<13:21,  3.80it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2395/4905 [10:00<09:39,  4.33it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2806/4905 [11:40<08:12,  4.26it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3305/4905 [13:20<05:56,  4.49it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3800/4905 [15:01<03:58,  4.63it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4336/4905 [16:41<01:57,  4.85it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4823/4905 [18:21<00:16,  4.86it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3623538974374532
Last average training loss at batch 1000 out of 4905: 1.3056407358497382
Last average training loss at batch 1500 out of 4905: 1.35800985494256
Last average training loss at batch 2000 out of 4905: 1.420562118753791
Last average training loss at batch 2500 out of 4905: 1.5530126565694808
Last average training loss at batch 3000 out of 4905: 1.6503364821523427
Last average training loss at batch 3500 out of 4905: 1.6912733673304319
Last average training loss at batch 4000 out of 4905: 1.778114488005638
Last average training loss at batch 4500 out of 4905: 1.4557002683281899
Last average training loss at batch 4905 out of 4905: 0.1342958238723932

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 411/702 [01:20<00:56,  5.13it/s][A
                                                 [A 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [19:04:40<13:07:08, 1098.34s/it]Average loss of 1.6243999004364014 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 58 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
  9%|â–‰         | 451/4905 [01:40<16:32,  4.49it/s][A
 19%|â–ˆâ–‰        | 930/4905 [03:20<14:12,  4.66it/s][A
 29%|â–ˆâ–ˆâ–‰       | 1422/4905 [05:01<12:10,  4.77it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1879/4905 [06:41<10:45,  4.69it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2394/4905 [08:21<08:37,  4.85it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2860/4905 [10:01<07:07,  4.78it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3330/4905 [11:41<05:31,  4.76it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3779/4905 [13:21<04:01,  4.67it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4229/4905 [15:01<02:26,  4.61it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4675/4905 [16:42<00:50,  4.56it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3620859341025353
Last average training loss at batch 1000 out of 4905: 1.3058453429192305
Last average training loss at batch 1500 out of 4905: 1.3581973672509193
Last average training loss at batch 2000 out of 4905: 1.4218882120847702
Last average training loss at batch 2500 out of 4905: 1.5524459716528654
Last average training loss at batch 3000 out of 4905: 1.6503799757659434
Last average training loss at batch 3500 out of 4905: 1.6900256165862084
Last average training loss at batch 4000 out of 4905: 1.7763929401934146
Last average training loss at batch 4500 out of 4905: 1.4563417526036502
Last average training loss at batch 4905 out of 4905: 0.1342441566949226

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 332/702 [01:20<01:29,  4.14it/s][A
                                                 [A 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [19:24:49<13:12:06, 1131.58s/it]Average loss of 1.6284174919128418 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 59 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
  9%|â–‰         | 431/4905 [01:40<17:19,  4.31it/s][A
 17%|â–ˆâ–‹        | 852/4905 [03:20<15:55,  4.24it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1257/4905 [05:00<14:39,  4.15it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 1704/4905 [06:41<12:31,  4.26it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2112/4905 [08:21<11:05,  4.19it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2503/4905 [10:01<09:46,  4.09it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2894/4905 [11:41<08:18,  4.03it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3298/4905 [13:22<06:38,  4.03it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3677/4905 [15:03<05:11,  3.94it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4046/4905 [16:43<03:42,  3.86it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4410/4905 [18:23<02:10,  3.79it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4782/4905 [20:05<00:32,  3.75it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3610859701037408
Last average training loss at batch 1000 out of 4905: 1.3062545855045318
Last average training loss at batch 1500 out of 4905: 1.358363985657692
Last average training loss at batch 2000 out of 4905: 1.4211813752651214
Last average training loss at batch 2500 out of 4905: 1.5523528953045607
Last average training loss at batch 3000 out of 4905: 1.6498882707208395
Last average training loss at batch 3500 out of 4905: 1.6918712415248156
Last average training loss at batch 4000 out of 4905: 1.7776456110775471
Last average training loss at batch 4500 out of 4905: 1.4554458330571651
Last average training loss at batch 4905 out of 4905: 0.1342524151614925

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 278/702 [01:20<02:02,  3.45it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 557/702 [02:40<00:41,  3.47it/s][A
                                                 [A 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [19:48:50<13:56:45, 1224.52s/it]Average loss of 1.624481439590454 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 60 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
  7%|â–‹         | 362/4905 [01:40<20:56,  3.61it/s][A
 15%|â–ˆâ–        | 720/4905 [03:20<19:26,  3.59it/s][A
 21%|â–ˆâ–ˆâ–       | 1054/4905 [05:01<18:31,  3.47it/s][A
 29%|â–ˆâ–ˆâ–Š       | 1398/4905 [06:41<16:54,  3.46it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 1696/4905 [08:21<16:17,  3.28it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1956/4905 [10:01<16:06,  3.05it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2212/4905 [11:41<15:32,  2.89it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2480/4905 [13:21<14:19,  2.82it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2719/4905 [15:01<13:34,  2.68it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3026/4905 [16:43<11:13,  2.79it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3280/4905 [18:23<09:59,  2.71it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3556/4905 [20:03<08:15,  2.73it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3801/4905 [21:44<06:58,  2.64it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4096/4905 [23:24<04:56,  2.73it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4366/4905 [25:04<03:18,  2.72it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4646/4905 [26:46<01:34,  2.73it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4883/4905 [28:26<00:08,  2.62it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3626549171954394
Last average training loss at batch 1000 out of 4905: 1.305705816909671
Last average training loss at batch 1500 out of 4905: 1.358114567115903
Last average training loss at batch 2000 out of 4905: 1.419988102838397
Last average training loss at batch 2500 out of 4905: 1.5537565259039403
Last average training loss at batch 3000 out of 4905: 1.6505309661626817
Last average training loss at batch 3500 out of 4905: 1.689659855425358
Last average training loss at batch 4000 out of 4905: 1.776846886664629
Last average training loss at batch 4500 out of 4905: 1.4553483827263116
Last average training loss at batch 4905 out of 4905: 0.13419631858879394

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 22%|â–ˆâ–ˆâ–       | 154/702 [01:20<04:47,  1.91it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 328/702 [02:42<03:03,  2.03it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 492/702 [04:03<01:43,  2.03it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 699/702 [05:25<00:01,  2.23it/s][A
                                                 [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [20:22:49<16:19:12, 1468.80s/it]Average loss of 1.6255429983139038 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 61 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
  4%|â–         | 215/4905 [01:40<36:24,  2.15it/s][A
  9%|â–‰         | 452/4905 [03:21<32:46,  2.26it/s][A
 14%|â–ˆâ–Ž        | 665/4905 [05:02<32:19,  2.19it/s][A
 18%|â–ˆâ–Š        | 902/4905 [06:43<29:33,  2.26it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1150/4905 [08:25<26:58,  2.32it/s][A
 28%|â–ˆâ–ˆâ–Š       | 1367/4905 [10:06<26:06,  2.26it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1596/4905 [11:47<24:21,  2.26it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1835/4905 [13:27<22:12,  2.30it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2096/4905 [15:07<19:32,  2.40it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2390/4905 [16:47<16:21,  2.56it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2638/4905 [18:29<14:59,  2.52it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2952/4905 [20:11<12:04,  2.70it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3321/4905 [21:51<08:49,  2.99it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3780/4905 [23:31<05:24,  3.47it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4205/4905 [25:11<03:08,  3.70it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4672/4905 [26:51<00:58,  3.99it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3612670562714337
Last average training loss at batch 1000 out of 4905: 1.305152933999896
Last average training loss at batch 1500 out of 4905: 1.3590524206608534
Last average training loss at batch 2000 out of 4905: 1.4207950685471296
Last average training loss at batch 2500 out of 4905: 1.5513399470895528
Last average training loss at batch 3000 out of 4905: 1.6507132309824228
Last average training loss at batch 3500 out of 4905: 1.6900283686071633
Last average training loss at batch 4000 out of 4905: 1.7775875803232193
Last average training loss at batch 4500 out of 4905: 1.4553819570988418
Last average training loss at batch 4905 out of 4905: 0.13427768846352894

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 393/702 [01:21<01:03,  4.85it/s][A
                                                 [A 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [20:52:45<16:58:32, 1566.99s/it]Average loss of 1.6223269701004028 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 62 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 11%|â–ˆ         | 521/4905 [01:40<14:02,  5.20it/s][A
 21%|â–ˆâ–ˆâ–       | 1048/4905 [03:20<12:16,  5.24it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1611/4905 [05:00<10:08,  5.42it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2266/4905 [06:40<07:30,  5.86it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2839/4905 [08:20<05:55,  5.81it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3433/4905 [10:00<04:11,  5.85it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4053/4905 [11:40<02:22,  5.97it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4668/4905 [13:20<00:39,  6.02it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3615530299693346
Last average training loss at batch 1000 out of 4905: 1.3053532082736492
Last average training loss at batch 1500 out of 4905: 1.3579376353621482
Last average training loss at batch 2000 out of 4905: 1.4210476990491152
Last average training loss at batch 2500 out of 4905: 1.553449050426483
Last average training loss at batch 3000 out of 4905: 1.649720200806856
Last average training loss at batch 3500 out of 4905: 1.689481201916933
Last average training loss at batch 4000 out of 4905: 1.777048273295164
Last average training loss at batch 4500 out of 4905: 1.4552152148038149
Last average training loss at batch 4905 out of 4905: 0.13435831285033387

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 494/702 [01:20<00:33,  6.14it/s][A
                                                 [A 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [21:08:38<14:35:44, 1382.74s/it]Average loss of 1.6242108345031738 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 63 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:18<?, ?it/s][A
 12%|â–ˆâ–        | 589/4905 [01:40<12:13,  5.89it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 1232/4905 [03:20<09:52,  6.20it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1851/4905 [05:00<08:12,  6.20it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2473/4905 [06:42<06:35,  6.15it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3034/4905 [08:22<05:14,  5.96it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3657/4905 [10:02<03:26,  6.05it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4276/4905 [11:42<01:43,  6.09it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4899/4905 [13:22<00:00,  6.13it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3630874007642269
Last average training loss at batch 1000 out of 4905: 1.307374426409602
Last average training loss at batch 1500 out of 4905: 1.3582747543603182
Last average training loss at batch 2000 out of 4905: 1.4200449753403663
Last average training loss at batch 2500 out of 4905: 1.552678070038557
Last average training loss at batch 3000 out of 4905: 1.6504966139644384
Last average training loss at batch 3500 out of 4905: 1.6893539836108684
Last average training loss at batch 4000 out of 4905: 1.778778650254011
Last average training loss at batch 4500 out of 4905: 1.4551623876243829
Last average training loss at batch 4905 out of 4905: 0.1342436011793295

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 476/702 [01:21<00:38,  5.88it/s][A
                                                 [A 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [21:23:57<12:46:52, 1243.58s/it]Average loss of 1.6248282194137573 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 64 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
 12%|â–ˆâ–        | 600/4905 [01:40<11:58,  5.99it/s][A
 25%|â–ˆâ–ˆâ–       | 1217/4905 [03:20<10:05,  6.09it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1823/4905 [05:00<08:27,  6.07it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2422/4905 [06:40<06:51,  6.04it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3039/4905 [08:21<05:07,  6.06it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3632/4905 [10:01<03:31,  6.01it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4260/4905 [11:42<01:45,  6.09it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4857/4905 [13:22<00:07,  6.05it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3614050787389278
Last average training loss at batch 1000 out of 4905: 1.3062817540168763
Last average training loss at batch 1500 out of 4905: 1.3581463991552591
Last average training loss at batch 2000 out of 4905: 1.4203695978820323
Last average training loss at batch 2500 out of 4905: 1.5529073480665683
Last average training loss at batch 3000 out of 4905: 1.6504240998327733
Last average training loss at batch 3500 out of 4905: 1.6904671702384948
Last average training loss at batch 4000 out of 4905: 1.7771200950592756
Last average training loss at batch 4500 out of 4905: 1.455849085494876
Last average training loss at batch 4905 out of 4905: 0.13423372334235792

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 470/702 [01:20<00:39,  5.84it/s][A
                                                 [A 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [21:39:28<11:29:48, 1149.68s/it]Average loss of 1.6247354745864868 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 65 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:18<?, ?it/s][A
 11%|â–ˆâ–        | 559/4905 [01:40<12:57,  5.59it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 1160/4905 [03:20<10:41,  5.83it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 1712/4905 [05:00<09:21,  5.69it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2309/4905 [06:40<07:28,  5.79it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2860/4905 [08:20<05:59,  5.69it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3460/4905 [10:00<04:09,  5.79it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3905/4905 [11:40<03:06,  5.35it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4486/4905 [13:21<01:16,  5.50it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3620441153496503
Last average training loss at batch 1000 out of 4905: 1.3059246085733176
Last average training loss at batch 1500 out of 4905: 1.358729357689619
Last average training loss at batch 2000 out of 4905: 1.4209888914227486
Last average training loss at batch 2500 out of 4905: 1.55274664118886
Last average training loss at batch 3000 out of 4905: 1.6507987142205238
Last average training loss at batch 3500 out of 4905: 1.691254578217864
Last average training loss at batch 4000 out of 4905: 1.775968574345112
Last average training loss at batch 4500 out of 4905: 1.4552814947366715
Last average training loss at batch 4905 out of 4905: 0.13410786655821688

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 436/702 [01:20<00:48,  5.44it/s][A
                                                 [A 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [21:56:06<10:44:12, 1104.35s/it]Average loss of 1.62533438205719 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 66 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
 12%|â–ˆâ–        | 602/4905 [01:40<11:55,  6.01it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 1155/4905 [03:20<10:54,  5.73it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1752/4905 [05:00<09:00,  5.84it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2331/4905 [06:40<07:22,  5.82it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2920/4905 [08:21<05:41,  5.81it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3472/4905 [10:01<04:10,  5.71it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4013/4905 [11:41<02:38,  5.61it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4626/4905 [13:21<00:48,  5.78it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3610864854753018
Last average training loss at batch 1000 out of 4905: 1.3060692361444235
Last average training loss at batch 1500 out of 4905: 1.3575134468972683
Last average training loss at batch 2000 out of 4905: 1.4212459103167057
Last average training loss at batch 2500 out of 4905: 1.5531637805104255
Last average training loss at batch 3000 out of 4905: 1.6513307735174894
Last average training loss at batch 3500 out of 4905: 1.6907517950385809
Last average training loss at batch 4000 out of 4905: 1.7769521538615227
Last average training loss at batch 4500 out of 4905: 1.4553640324771404
Last average training loss at batch 4905 out of 4905: 0.13415477761617128

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:15<?, ?it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 462/702 [01:20<00:41,  5.77it/s][A
                                                 [A 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [22:12:23<10:04:05, 1066.03s/it]Average loss of 1.6245986223220825 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 67 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 11%|â–ˆ         | 551/4905 [01:40<13:11,  5.50it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 1127/4905 [03:20<11:08,  5.65it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1754/4905 [05:00<08:51,  5.93it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2346/4905 [06:40<07:12,  5.92it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2993/4905 [08:20<05:12,  6.12it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3604/4905 [10:00<03:32,  6.11it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4200/4905 [11:40<01:56,  6.06it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4802/4905 [13:20<00:17,  6.05it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3605806567966938
Last average training loss at batch 1000 out of 4905: 1.3063549633175136
Last average training loss at batch 1500 out of 4905: 1.357560723990202
Last average training loss at batch 2000 out of 4905: 1.4221566760092974
Last average training loss at batch 2500 out of 4905: 1.552908720523119
Last average training loss at batch 3000 out of 4905: 1.6515413365364076
Last average training loss at batch 3500 out of 4905: 1.6911775064468384
Last average training loss at batch 4000 out of 4905: 1.7765772986114026
Last average training loss at batch 4500 out of 4905: 1.45538113476336
Last average training loss at batch 4905 out of 4905: 0.13425420925522433

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 448/702 [01:20<00:45,  5.59it/s][A
                                                 [A 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [22:27:59<9:24:54, 1027.11s/it] Average loss of 1.6253710985183716 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 68 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
  9%|â–‰         | 437/4905 [01:40<17:03,  4.36it/s][A
 22%|â–ˆâ–ˆâ–       | 1057/4905 [03:20<11:46,  5.44it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 1684/4905 [05:00<09:13,  5.82it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2287/4905 [06:40<07:23,  5.90it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2886/4905 [08:20<05:40,  5.93it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3490/4905 [10:00<03:57,  5.97it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4141/4905 [11:40<02:04,  6.14it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4796/4905 [13:20<00:17,  6.27it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3620967460870743
Last average training loss at batch 1000 out of 4905: 1.3052379557937384
Last average training loss at batch 1500 out of 4905: 1.3569106817096472
Last average training loss at batch 2000 out of 4905: 1.420182036012411
Last average training loss at batch 2500 out of 4905: 1.552122267305851
Last average training loss at batch 3000 out of 4905: 1.6495092338472606
Last average training loss at batch 3500 out of 4905: 1.6898538713157176
Last average training loss at batch 4000 out of 4905: 1.7769444340467453
Last average training loss at batch 4500 out of 4905: 1.45542286413908
Last average training loss at batch 4905 out of 4905: 0.13419771681138135

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:16<?, ?it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 521/702 [01:20<00:27,  6.50it/s][A
                                                 [A 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [22:43:31<8:52:34, 998.59s/it] Average loss of 1.6253072023391724 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 69 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
 13%|â–ˆâ–Ž        | 640/4905 [01:40<11:08,  6.38it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1284/4905 [03:20<09:24,  6.41it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1969/4905 [05:00<07:24,  6.61it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2602/4905 [06:40<05:54,  6.50it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3260/4905 [08:20<04:12,  6.52it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3886/4905 [10:00<02:38,  6.43it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4502/4905 [11:40<01:03,  6.34it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3613560284078121
Last average training loss at batch 1000 out of 4905: 1.3069570163488389
Last average training loss at batch 1500 out of 4905: 1.3569495639652014
Last average training loss at batch 2000 out of 4905: 1.4208011516928674
Last average training loss at batch 2500 out of 4905: 1.5532475361824036
Last average training loss at batch 3000 out of 4905: 1.650774240359664
Last average training loss at batch 3500 out of 4905: 1.6903888149261475
Last average training loss at batch 4000 out of 4905: 1.7779897055327891
Last average training loss at batch 4500 out of 4905: 1.4557279926687479
Last average training loss at batch 4905 out of 4905: 0.13424228558603535

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 444/702 [01:20<00:46,  5.55it/s][A
                                                 [A 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [22:58:17<8:18:28, 964.80s/it]Average loss of 1.6248120069503784 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 70 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
 12%|â–ˆâ–        | 599/4905 [01:40<11:59,  5.98it/s][A
 25%|â–ˆâ–ˆâ–       | 1226/4905 [03:20<09:59,  6.14it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1844/4905 [05:00<08:17,  6.16it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2489/4905 [06:40<06:25,  6.27it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3125/4905 [08:20<04:42,  6.30it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3784/4905 [10:00<02:55,  6.40it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4395/4905 [11:40<01:20,  6.30it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3615846899747848
Last average training loss at batch 1000 out of 4905: 1.3059219501316548
Last average training loss at batch 1500 out of 4905: 1.3580947628617286
Last average training loss at batch 2000 out of 4905: 1.4202405878156423
Last average training loss at batch 2500 out of 4905: 1.5529926072955131
Last average training loss at batch 3000 out of 4905: 1.6499595549553632
Last average training loss at batch 3500 out of 4905: 1.6907014161646365
Last average training loss at batch 4000 out of 4905: 1.7778184478580952
Last average training loss at batch 4500 out of 4905: 1.456257792890072
Last average training loss at batch 4905 out of 4905: 0.13424602290055318

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 530/702 [01:20<00:25,  6.62it/s][A
                                                 [A 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [23:13:02<7:50:27, 940.90s/it]Average loss of 1.6248492002487183 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 71 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
 13%|â–ˆâ–Ž        | 620/4905 [01:40<11:31,  6.20it/s][A
 26%|â–ˆâ–ˆâ–‹       | 1290/4905 [03:20<09:17,  6.49it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1865/4905 [05:00<08:14,  6.15it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2532/4905 [06:40<06:13,  6.36it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3155/4905 [08:20<04:37,  6.31it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3791/4905 [10:00<02:56,  6.32it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4443/4905 [11:40<01:12,  6.39it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3627859057337046
Last average training loss at batch 1000 out of 4905: 1.306750392973423
Last average training loss at batch 1500 out of 4905: 1.3576041089594364
Last average training loss at batch 2000 out of 4905: 1.4213043226897717
Last average training loss at batch 2500 out of 4905: 1.5523846697211265
Last average training loss at batch 3000 out of 4905: 1.6505215966552496
Last average training loss at batch 3500 out of 4905: 1.691096210733056
Last average training loss at batch 4000 out of 4905: 1.7770308701992035
Last average training loss at batch 4500 out of 4905: 1.4562559468448162
Last average training loss at batch 4905 out of 4905: 0.13426249627855574

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 528/702 [01:20<00:26,  6.56it/s][A
                                                 [A 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [23:27:41<7:25:46, 922.30s/it]Average loss of 1.6282504796981812 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 72 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
 12%|â–ˆâ–        | 567/4905 [01:40<12:48,  5.64it/s][A
 18%|â–ˆâ–Š        | 902/4905 [03:21<15:36,  4.27it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 1482/4905 [05:01<11:29,  4.97it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2152/4905 [06:41<08:07,  5.65it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2726/4905 [08:21<06:23,  5.68it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3334/4905 [10:01<04:30,  5.81it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3994/4905 [11:41<02:30,  6.07it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4654/4905 [13:21<00:40,  6.24it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3629948099404574
Last average training loss at batch 1000 out of 4905: 1.3046057379096747
Last average training loss at batch 1500 out of 4905: 1.3593342427313329
Last average training loss at batch 2000 out of 4905: 1.4199919970333577
Last average training loss at batch 2500 out of 4905: 1.5543559148907662
Last average training loss at batch 3000 out of 4905: 1.6497439163327217
Last average training loss at batch 3500 out of 4905: 1.691968097433448
Last average training loss at batch 4000 out of 4905: 1.776474615395069
Last average training loss at batch 4500 out of 4905: 1.4553616998940706
Last average training loss at batch 4905 out of 4905: 0.13418444529950435

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 550/702 [01:20<00:22,  6.85it/s][A
                                                 [A 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [23:43:22<7:13:02, 927.95s/it]Average loss of 1.6225471496582031 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 73 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
 13%|â–ˆâ–Ž        | 651/4905 [01:40<10:53,  6.51it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1307/4905 [03:20<09:10,  6.54it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1964/4905 [05:00<07:29,  6.55it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2651/4905 [06:40<05:37,  6.67it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3327/4905 [08:20<03:55,  6.70it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3997/4905 [10:00<02:15,  6.70it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4681/4905 [11:40<00:33,  6.74it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3616921678334475
Last average training loss at batch 1000 out of 4905: 1.3068827618211507
Last average training loss at batch 1500 out of 4905: 1.357937967851758
Last average training loss at batch 2000 out of 4905: 1.4207938684076071
Last average training loss at batch 2500 out of 4905: 1.552509325698018
Last average training loss at batch 3000 out of 4905: 1.6493258974552154
Last average training loss at batch 3500 out of 4905: 1.6908964788019658
Last average training loss at batch 4000 out of 4905: 1.7769208005666732
Last average training loss at batch 4500 out of 4905: 1.4549826635420322
Last average training loss at batch 4905 out of 4905: 0.1341501095031258

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 550/702 [01:20<00:22,  6.86it/s][A
                                                 [A 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [23:57:15<6:44:46, 899.48s/it]Average loss of 1.6257531642913818 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 74 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 13%|â–ˆâ–Ž        | 655/4905 [01:40<10:49,  6.55it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1314/4905 [03:20<09:06,  6.57it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1976/4905 [05:00<07:24,  6.59it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2628/4905 [06:40<05:47,  6.56it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3300/4905 [08:20<04:02,  6.61it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3984/4905 [10:00<02:17,  6.69it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4668/4905 [11:40<00:35,  6.74it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3628634592592717
Last average training loss at batch 1000 out of 4905: 1.306129286468029
Last average training loss at batch 1500 out of 4905: 1.3576823954582213
Last average training loss at batch 2000 out of 4905: 1.4203918692022561
Last average training loss at batch 2500 out of 4905: 1.55374749442935
Last average training loss at batch 3000 out of 4905: 1.651266377940774
Last average training loss at batch 3500 out of 4905: 1.6895938740968703
Last average training loss at batch 4000 out of 4905: 1.7768902082443236
Last average training loss at batch 4500 out of 4905: 1.4548825820684432
Last average training loss at batch 4905 out of 4905: 0.1341624580194583

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 550/702 [01:20<00:22,  6.85it/s][A
                                                 [A 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [24:11:13<6:21:45, 880.99s/it]Average loss of 1.6252387762069702 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 75 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 13%|â–ˆâ–Ž        | 654/4905 [01:40<10:50,  6.53it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1279/4905 [03:20<09:29,  6.37it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1948/4905 [05:00<07:33,  6.51it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2611/4905 [06:40<05:49,  6.56it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3288/4905 [08:20<04:03,  6.63it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3980/4905 [10:00<02:17,  6.73it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4671/4905 [11:40<00:34,  6.78it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3626326466947793
Last average training loss at batch 1000 out of 4905: 1.305214824974537
Last average training loss at batch 1500 out of 4905: 1.356660769984126
Last average training loss at batch 2000 out of 4905: 1.4208510612249374
Last average training loss at batch 2500 out of 4905: 1.5516108267456292
Last average training loss at batch 3000 out of 4905: 1.650375762939453
Last average training loss at batch 3500 out of 4905: 1.6902934630662203
Last average training loss at batch 4000 out of 4905: 1.776508823543787
Last average training loss at batch 4500 out of 4905: 1.4545824552029372
Last average training loss at batch 4905 out of 4905: 0.13428215930859247

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 554/702 [01:20<00:21,  6.92it/s][A
                                                 [A 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [24:25:10<6:01:33, 867.72s/it]Average loss of 1.6250090599060059 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 76 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:16<?, ?it/s][A
 13%|â–ˆâ–Ž        | 660/4905 [01:40<10:43,  6.60it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1307/4905 [03:20<09:11,  6.52it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1958/4905 [05:00<07:32,  6.51it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2616/4905 [06:40<05:50,  6.54it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3283/4905 [08:20<04:06,  6.58it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3932/4905 [10:00<02:28,  6.55it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4612/4905 [11:40<00:44,  6.63it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3620009203255177
Last average training loss at batch 1000 out of 4905: 1.306879249960184
Last average training loss at batch 1500 out of 4905: 1.3573588573485613
Last average training loss at batch 2000 out of 4905: 1.420483565032482
Last average training loss at batch 2500 out of 4905: 1.5523334493041039
Last average training loss at batch 3000 out of 4905: 1.6509027498364448
Last average training loss at batch 3500 out of 4905: 1.691245870217681
Last average training loss at batch 4000 out of 4905: 1.7761566686332226
Last average training loss at batch 4500 out of 4905: 1.4562118317931891
Last average training loss at batch 4905 out of 4905: 0.1342382098268053

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 476/702 [01:20<00:38,  5.93it/s][A
                                                 [A 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [24:39:34<5:46:39, 866.63s/it]Average loss of 1.6248650550842285 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 77 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 13%|â–ˆâ–Ž        | 640/4905 [01:40<11:06,  6.40it/s][A
 26%|â–ˆâ–ˆâ–‹       | 1299/4905 [03:20<09:14,  6.51it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1899/4905 [05:00<07:59,  6.27it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2354/4905 [06:40<07:36,  5.59it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2799/4905 [08:20<06:46,  5.18it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3467/4905 [10:00<04:12,  5.69it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4062/4905 [11:40<02:26,  5.77it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4612/4905 [13:20<00:51,  5.68it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362852003440261
Last average training loss at batch 1000 out of 4905: 1.3050769468843937
Last average training loss at batch 1500 out of 4905: 1.3578068173080682
Last average training loss at batch 2000 out of 4905: 1.421050618737936
Last average training loss at batch 2500 out of 4905: 1.5528527026325465
Last average training loss at batch 3000 out of 4905: 1.649464886188507
Last average training loss at batch 3500 out of 4905: 1.6904597806483508
Last average training loss at batch 4000 out of 4905: 1.776744410574436
Last average training loss at batch 4500 out of 4905: 1.4553058626502753
Last average training loss at batch 4905 out of 4905: 0.13428327093369857

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:18<?, ?it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 504/702 [01:20<00:31,  6.28it/s][A
                                                 [A 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [24:55:28<5:42:16, 892.89s/it]Average loss of 1.6250838041305542 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 78 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:18<?, ?it/s][A
 13%|â–ˆâ–Ž        | 628/4905 [01:40<11:21,  6.27it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1252/4905 [03:20<09:44,  6.25it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1855/4905 [05:00<08:16,  6.15it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2467/4905 [06:40<06:37,  6.14it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3093/4905 [08:20<04:53,  6.18it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3756/4905 [10:00<03:01,  6.33it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4361/4905 [11:40<01:27,  6.23it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3622996643185616
Last average training loss at batch 1000 out of 4905: 1.3067994491755963
Last average training loss at batch 1500 out of 4905: 1.359350053831935
Last average training loss at batch 2000 out of 4905: 1.421289874240756
Last average training loss at batch 2500 out of 4905: 1.55185641682148
Last average training loss at batch 3000 out of 4905: 1.6513257474005223
Last average training loss at batch 3500 out of 4905: 1.691679701194167
Last average training loss at batch 4000 out of 4905: 1.777698883831501
Last average training loss at batch 4500 out of 4905: 1.4557274908572435
Last average training loss at batch 4905 out of 4905: 0.13417138860738484

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 537/702 [01:20<00:24,  6.71it/s][A
                                                 [A 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [25:10:14<5:26:37, 890.81s/it]Average loss of 1.6231602430343628 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 79 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 13%|â–ˆâ–Ž        | 661/4905 [01:40<10:44,  6.58it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 1245/4905 [03:20<09:55,  6.14it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1916/4905 [05:00<07:46,  6.40it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2558/4905 [06:40<06:06,  6.41it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3196/4905 [08:20<04:27,  6.39it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3858/4905 [10:00<02:41,  6.47it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4511/4905 [11:40<01:00,  6.49it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3627747424542904
Last average training loss at batch 1000 out of 4905: 1.304673909932375
Last average training loss at batch 1500 out of 4905: 1.3566792244166135
Last average training loss at batch 2000 out of 4905: 1.4214630913883448
Last average training loss at batch 2500 out of 4905: 1.5515395348966121
Last average training loss at batch 3000 out of 4905: 1.6503467288017273
Last average training loss at batch 3500 out of 4905: 1.6914596395045518
Last average training loss at batch 4000 out of 4905: 1.7768907232284545
Last average training loss at batch 4500 out of 4905: 1.4562120082527399
Last average training loss at batch 4905 out of 4905: 0.13421720318898755

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:13<?, ?it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 547/702 [01:21<00:23,  6.73it/s][A
                                                 [A 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [25:24:37<5:08:48, 882.33s/it]Average loss of 1.6230007410049438 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 80 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
 13%|â–ˆâ–Ž        | 646/4905 [01:40<10:59,  6.46it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1283/4905 [03:20<09:25,  6.40it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1942/4905 [05:00<07:36,  6.49it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2587/4905 [06:40<05:58,  6.47it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3260/4905 [08:20<04:10,  6.56it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3917/4905 [10:00<02:30,  6.56it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4597/4905 [11:40<00:46,  6.64it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362419320821762
Last average training loss at batch 1000 out of 4905: 1.3059728771150112
Last average training loss at batch 1500 out of 4905: 1.358049587622285
Last average training loss at batch 2000 out of 4905: 1.4205260121524335
Last average training loss at batch 2500 out of 4905: 1.5525377898961306
Last average training loss at batch 3000 out of 4905: 1.6499385146051646
Last average training loss at batch 3500 out of 4905: 1.692044151544571
Last average training loss at batch 4000 out of 4905: 1.7763363420069218
Last average training loss at batch 4500 out of 4905: 1.4539900740683078
Last average training loss at batch 4905 out of 4905: 0.13419294848384963

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 583/702 [01:20<00:16,  7.22it/s][A
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [25:38:42<4:50:22, 871.14s/it]Average loss of 1.6259008646011353 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 81 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
 13%|â–ˆâ–Ž        | 656/4905 [01:40<10:48,  6.56it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1319/4905 [03:20<09:03,  6.60it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1996/4905 [05:00<07:15,  6.68it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2660/4905 [06:40<05:37,  6.66it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3337/4905 [08:20<03:54,  6.69it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4000/4905 [10:00<02:15,  6.67it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4691/4905 [11:40<00:31,  6.75it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.360227723568678
Last average training loss at batch 1000 out of 4905: 1.305242076098919
Last average training loss at batch 1500 out of 4905: 1.3574185676276684
Last average training loss at batch 2000 out of 4905: 1.4207422751039267
Last average training loss at batch 2500 out of 4905: 1.5516367732286453
Last average training loss at batch 3000 out of 4905: 1.6495952143073083
Last average training loss at batch 3500 out of 4905: 1.6901937759816648
Last average training loss at batch 4000 out of 4905: 1.7777855631113053
Last average training loss at batch 4500 out of 4905: 1.4551091545820236
Last average training loss at batch 4905 out of 4905: 0.13413587247043332

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 574/702 [01:20<00:17,  7.16it/s][A
                                                 [A 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [25:52:35<4:32:17, 859.87s/it]Average loss of 1.6243828535079956 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 82 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 13%|â–ˆâ–Ž        | 656/4905 [01:40<10:48,  6.55it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 1272/4905 [03:20<09:34,  6.32it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1948/4905 [05:00<07:33,  6.52it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2637/4905 [06:40<05:40,  6.66it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3320/4905 [08:20<03:55,  6.72it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3998/4905 [10:00<02:14,  6.74it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4684/4905 [11:40<00:32,  6.78it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3603105045557022
Last average training loss at batch 1000 out of 4905: 1.3065061489790677
Last average training loss at batch 1500 out of 4905: 1.357231243416667
Last average training loss at batch 2000 out of 4905: 1.4214477726817132
Last average training loss at batch 2500 out of 4905: 1.5532093529254198
Last average training loss at batch 3000 out of 4905: 1.650604201823473
Last average training loss at batch 3500 out of 4905: 1.6906817218363286
Last average training loss at batch 4000 out of 4905: 1.77658442401886
Last average training loss at batch 4500 out of 4905: 1.4567280543595553
Last average training loss at batch 4905 out of 4905: 0.1342384453486959

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:17<?, ?it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 556/702 [01:20<00:21,  6.92it/s][A
                                                 [A 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [26:06:29<4:15:36, 852.03s/it]Average loss of 1.6234362125396729 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 83 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
 14%|â–ˆâ–Ž        | 664/4905 [01:40<10:38,  6.64it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1323/4905 [03:20<09:02,  6.61it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1987/4905 [05:00<07:20,  6.62it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2584/4905 [06:40<06:04,  6.36it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3251/4905 [08:20<04:15,  6.47it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3918/4905 [10:00<02:30,  6.54it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4599/4905 [11:40<00:46,  6.63it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3624687655717134
Last average training loss at batch 1000 out of 4905: 1.3048957142531872
Last average training loss at batch 1500 out of 4905: 1.3573388292044402
Last average training loss at batch 2000 out of 4905: 1.420639454498887
Last average training loss at batch 2500 out of 4905: 1.5529365826398134
Last average training loss at batch 3000 out of 4905: 1.6513734090179204
Last average training loss at batch 3500 out of 4905: 1.6893974906802178
Last average training loss at batch 4000 out of 4905: 1.7767047563195228
Last average training loss at batch 4500 out of 4905: 1.4562778157144785
Last average training loss at batch 4905 out of 4905: 0.1342270138127602

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:12<?, ?it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 511/702 [01:20<00:29,  6.38it/s][A
                                                 [A 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [26:20:45<4:01:43, 853.14s/it]Average loss of 1.6260619163513184 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 84 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 13%|â–ˆâ–Ž        | 636/4905 [01:40<11:12,  6.35it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1301/4905 [03:20<09:12,  6.52it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1965/4905 [05:00<07:27,  6.57it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2584/4905 [06:40<06:01,  6.42it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3157/4905 [08:20<04:43,  6.17it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3655/4905 [10:00<03:36,  5.76it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4095/4905 [11:40<02:32,  5.32it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4719/4905 [13:20<00:33,  5.61it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3622512314468622
Last average training loss at batch 1000 out of 4905: 1.3064194585680962
Last average training loss at batch 1500 out of 4905: 1.3567676846385002
Last average training loss at batch 2000 out of 4905: 1.4197582562416793
Last average training loss at batch 2500 out of 4905: 1.5521433581858874
Last average training loss at batch 3000 out of 4905: 1.6505600580871105
Last average training loss at batch 3500 out of 4905: 1.6904971902668475
Last average training loss at batch 4000 out of 4905: 1.7768042783141136
Last average training loss at batch 4500 out of 4905: 1.4562558306008577
Last average training loss at batch 4905 out of 4905: 0.13415195588611073

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 527/702 [01:20<00:26,  6.57it/s][A
                                                 [A 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [26:36:15<3:53:40, 876.28s/it]Average loss of 1.6253647804260254 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 85 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 13%|â–ˆâ–Ž        | 654/4905 [01:40<10:50,  6.53it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1304/4905 [03:20<09:13,  6.51it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1956/4905 [05:00<07:32,  6.51it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2600/4905 [06:40<05:55,  6.48it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3284/4905 [08:20<04:05,  6.61it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3940/4905 [10:00<02:26,  6.59it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4594/4905 [11:40<00:47,  6.57it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.361874157756567
Last average training loss at batch 1000 out of 4905: 1.3048500885665417
Last average training loss at batch 1500 out of 4905: 1.3574903510659933
Last average training loss at batch 2000 out of 4905: 1.4204856121391058
Last average training loss at batch 2500 out of 4905: 1.5522386872172356
Last average training loss at batch 3000 out of 4905: 1.6505771881490945
Last average training loss at batch 3500 out of 4905: 1.6900117771178484
Last average training loss at batch 4000 out of 4905: 1.7773272043466568
Last average training loss at batch 4500 out of 4905: 1.456437283322215
Last average training loss at batch 4905 out of 4905: 0.13432388978266935

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 538/702 [01:20<00:24,  6.72it/s][A
                                                 [A 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [26:50:26<3:37:11, 868.76s/it]Average loss of 1.6226507425308228 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 86 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
 13%|â–ˆâ–Ž        | 662/4905 [01:40<10:41,  6.61it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1316/4905 [03:20<09:06,  6.57it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1985/4905 [05:00<07:21,  6.61it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2671/4905 [06:40<05:32,  6.71it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3365/4905 [08:20<03:46,  6.79it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4045/4905 [10:00<02:06,  6.79it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4718/4905 [11:40<00:27,  6.77it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3625577172338963
Last average training loss at batch 1000 out of 4905: 1.3058694618344306
Last average training loss at batch 1500 out of 4905: 1.357698481425643
Last average training loss at batch 2000 out of 4905: 1.4203656447529793
Last average training loss at batch 2500 out of 4905: 1.5529764451235533
Last average training loss at batch 3000 out of 4905: 1.649766737550497
Last average training loss at batch 3500 out of 4905: 1.6906758811920881
Last average training loss at batch 4000 out of 4905: 1.7775525271892547
Last average training loss at batch 4500 out of 4905: 1.455414713844657
Last average training loss at batch 4905 out of 4905: 0.13413837155010114

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 526/702 [01:20<00:26,  6.55it/s][A
                                                 [A 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [27:04:19<3:20:12, 858.04s/it]Average loss of 1.626997947692871 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 87 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:17<?, ?it/s][A
 14%|â–ˆâ–        | 676/4905 [01:40<10:25,  6.76it/s][A
 28%|â–ˆâ–ˆâ–Š       | 1356/4905 [03:20<08:43,  6.78it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2036/4905 [05:00<07:02,  6.79it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2721/4905 [06:40<05:20,  6.81it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3395/4905 [08:21<03:43,  6.76it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4067/4905 [10:01<02:04,  6.75it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4734/4905 [11:41<00:25,  6.72it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.361277259245515
Last average training loss at batch 1000 out of 4905: 1.3053536497652531
Last average training loss at batch 1500 out of 4905: 1.358297631084919
Last average training loss at batch 2000 out of 4905: 1.420451502919197
Last average training loss at batch 2500 out of 4905: 1.5512612469792366
Last average training loss at batch 3000 out of 4905: 1.6520845807641744
Last average training loss at batch 3500 out of 4905: 1.6899243779182433
Last average training loss at batch 4000 out of 4905: 1.7773362067341805
Last average training loss at batch 4500 out of 4905: 1.4552291594594717
Last average training loss at batch 4905 out of 4905: 0.1341612394158629

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 601/702 [01:20<00:13,  7.48it/s][A
                                                 [A 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [27:18:02<3:03:35, 847.32s/it]Average loss of 1.6248923540115356 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 88 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
 13%|â–ˆâ–Ž        | 655/4905 [01:40<10:49,  6.54it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1324/4905 [03:20<09:00,  6.63it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1993/4905 [05:00<07:17,  6.65it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2668/4905 [06:40<05:34,  6.69it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3253/4905 [08:20<04:18,  6.38it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3911/4905 [10:00<02:34,  6.45it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4575/4905 [11:40<00:50,  6.51it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3604873528331518
Last average training loss at batch 1000 out of 4905: 1.3055965292602778
Last average training loss at batch 1500 out of 4905: 1.3567240780740977
Last average training loss at batch 2000 out of 4905: 1.4193846483826638
Last average training loss at batch 2500 out of 4905: 1.5532810668200254
Last average training loss at batch 3000 out of 4905: 1.6502696706652642
Last average training loss at batch 3500 out of 4905: 1.6909913904070855
Last average training loss at batch 4000 out of 4905: 1.77659208086133
Last average training loss at batch 4500 out of 4905: 1.4562529481053352
Last average training loss at batch 4905 out of 4905: 0.13414912171866916

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 539/702 [01:20<00:24,  6.71it/s][A
                                                 [A 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [27:32:22<2:50:14, 851.22s/it]Average loss of 1.6240525245666504 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 89 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
 13%|â–ˆâ–Ž        | 661/4905 [01:40<10:42,  6.61it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1341/4905 [03:20<08:50,  6.72it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1980/4905 [05:00<07:25,  6.56it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2658/4905 [06:40<05:37,  6.65it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3345/4905 [08:20<03:51,  6.73it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4024/4905 [10:00<02:10,  6.75it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4669/4905 [11:40<00:35,  6.65it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.361345743894577
Last average training loss at batch 1000 out of 4905: 1.305045584321022
Last average training loss at batch 1500 out of 4905: 1.358360812574625
Last average training loss at batch 2000 out of 4905: 1.4212219820469618
Last average training loss at batch 2500 out of 4905: 1.5534898320138455
Last average training loss at batch 3000 out of 4905: 1.6511190493255854
Last average training loss at batch 3500 out of 4905: 1.6905908476561309
Last average training loss at batch 4000 out of 4905: 1.7776074070334436
Last average training loss at batch 4500 out of 4905: 1.4558347386866808
Last average training loss at batch 4905 out of 4905: 0.13423459871494564

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 556/702 [01:20<00:21,  6.92it/s][A
                                                 [A 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [27:46:17<2:35:09, 846.33s/it]Average loss of 1.6248456239700317 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 90 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
 14%|â–ˆâ–        | 685/4905 [01:40<10:16,  6.84it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1339/4905 [03:20<08:55,  6.66it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2025/4905 [05:00<07:06,  6.75it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2700/4905 [06:40<05:26,  6.75it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3378/4905 [08:20<03:45,  6.76it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4057/4905 [10:00<02:05,  6.77it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4738/4905 [11:40<00:24,  6.78it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3608222030252217
Last average training loss at batch 1000 out of 4905: 1.3053993414342404
Last average training loss at batch 1500 out of 4905: 1.357112809881568
Last average training loss at batch 2000 out of 4905: 1.421406316936016
Last average training loss at batch 2500 out of 4905: 1.5506278802752496
Last average training loss at batch 3000 out of 4905: 1.6497989774495363
Last average training loss at batch 3500 out of 4905: 1.6904210302382707
Last average training loss at batch 4000 out of 4905: 1.7772611244022847
Last average training loss at batch 4500 out of 4905: 1.4558534845560789
Last average training loss at batch 4905 out of 4905: 0.1342185656229655

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 561/702 [01:20<00:20,  7.00it/s][A
                                                 [A 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [28:00:02<2:20:00, 840.01s/it]Average loss of 1.62559175491333 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 91 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:14<?, ?it/s][A
 14%|â–ˆâ–Ž        | 666/4905 [01:40<10:37,  6.65it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 1248/4905 [03:20<09:56,  6.13it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1927/4905 [05:01<07:43,  6.43it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2603/4905 [06:41<05:51,  6.56it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3291/4905 [08:21<04:01,  6.67it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3824/4905 [10:02<02:54,  6.20it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4288/4905 [11:42<01:48,  5.69it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3615944242924451
Last average training loss at batch 1000 out of 4905: 1.305646083176136
Last average training loss at batch 1500 out of 4905: 1.3575243060141802
Last average training loss at batch 2000 out of 4905: 1.4207027980089189
Last average training loss at batch 2500 out of 4905: 1.5537887367606162
Last average training loss at batch 3000 out of 4905: 1.6493630309849978
Last average training loss at batch 3500 out of 4905: 1.691009978145361
Last average training loss at batch 4000 out of 4905: 1.775827550828457
Last average training loss at batch 4500 out of 4905: 1.4554928678572179
Last average training loss at batch 4905 out of 4905: 0.13436872812403816

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 572/702 [01:21<00:18,  7.06it/s][A
                                                 [A 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [28:14:54<2:08:21, 855.72s/it]Average loss of 1.6253284215927124 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 92 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 13%|â–ˆâ–Ž        | 653/4905 [01:40<10:51,  6.52it/s][A
 26%|â–ˆâ–ˆâ–‹       | 1296/4905 [03:20<09:18,  6.47it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1959/4905 [05:00<07:30,  6.54it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2639/4905 [06:40<05:41,  6.64it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3310/4905 [08:20<03:59,  6.66it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3981/4905 [10:00<02:18,  6.68it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4655/4905 [11:40<00:37,  6.69it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3629947823882103
Last average training loss at batch 1000 out of 4905: 1.3038480293005705
Last average training loss at batch 1500 out of 4905: 1.3577332490980625
Last average training loss at batch 2000 out of 4905: 1.4203491644859314
Last average training loss at batch 2500 out of 4905: 1.5523393514603376
Last average training loss at batch 3000 out of 4905: 1.6512278428375722
Last average training loss at batch 3500 out of 4905: 1.6914061107486487
Last average training loss at batch 4000 out of 4905: 1.7770442759394647
Last average training loss at batch 4500 out of 4905: 1.455673108741641
Last average training loss at batch 4905 out of 4905: 0.13429106382024641

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:14<?, ?it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 592/702 [01:20<00:14,  7.40it/s][A
                                                 [A 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [28:28:46<1:53:07, 848.39s/it]Average loss of 1.6253200769424438 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 93 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:11<?, ?it/s][A
 12%|â–ˆâ–        | 581/4905 [01:40<12:24,  5.81it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 1247/4905 [03:20<09:39,  6.31it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1937/4905 [05:00<07:31,  6.58it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2615/4905 [06:40<05:44,  6.65it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3265/4905 [08:20<04:08,  6.60it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3909/4905 [10:00<02:32,  6.54it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4536/4905 [11:40<00:57,  6.45it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3609161187410355
Last average training loss at batch 1000 out of 4905: 1.3052030108720065
Last average training loss at batch 1500 out of 4905: 1.3578052932620048
Last average training loss at batch 2000 out of 4905: 1.4199690418839455
Last average training loss at batch 2500 out of 4905: 1.552837760820985
Last average training loss at batch 3000 out of 4905: 1.649138041958213
Last average training loss at batch 3500 out of 4905: 1.6904751984477042
Last average training loss at batch 4000 out of 4905: 1.7770799633860588
Last average training loss at batch 4500 out of 4905: 1.4553483762741088
Last average training loss at batch 4905 out of 4905: 0.1341716204123395

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:15<?, ?it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 534/702 [01:20<00:25,  6.66it/s][A
                                                 [A 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [28:43:06<1:39:24, 852.09s/it]Average loss of 1.62721586227417 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 94 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:10<?, ?it/s][A
 13%|â–ˆâ–Ž        | 618/4905 [01:40<11:35,  6.16it/s][A
 26%|â–ˆâ–ˆâ–‹       | 1295/4905 [03:20<09:14,  6.52it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1961/4905 [05:00<07:27,  6.58it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2632/4905 [06:40<05:42,  6.63it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3309/4905 [08:20<03:58,  6.68it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3986/4905 [10:00<02:16,  6.71it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4640/4905 [11:40<00:39,  6.65it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3629520395696164
Last average training loss at batch 1000 out of 4905: 1.305624064937234
Last average training loss at batch 1500 out of 4905: 1.3580987005829812
Last average training loss at batch 2000 out of 4905: 1.420871275022626
Last average training loss at batch 2500 out of 4905: 1.5538630135655402
Last average training loss at batch 3000 out of 4905: 1.6498742103874684
Last average training loss at batch 3500 out of 4905: 1.6902462946772576
Last average training loss at batch 4000 out of 4905: 1.775568727105856
Last average training loss at batch 4500 out of 4905: 1.4555094915777445
Last average training loss at batch 4905 out of 4905: 0.13420328980465548

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 577/702 [01:20<00:17,  7.17it/s][A
                                                 [A 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [28:57:04<1:24:46, 847.82s/it]Average loss of 1.6277062892913818 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 95 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 13%|â–ˆâ–Ž        | 659/4905 [01:40<10:44,  6.59it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1314/4905 [03:20<09:06,  6.57it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2003/4905 [05:00<07:12,  6.71it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2658/4905 [06:40<05:38,  6.65it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3345/4905 [08:20<03:51,  6.73it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4007/4905 [10:00<02:14,  6.69it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4696/4905 [11:40<00:30,  6.75it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.36166029278934
Last average training loss at batch 1000 out of 4905: 1.306899780213833
Last average training loss at batch 1500 out of 4905: 1.357702277228236
Last average training loss at batch 2000 out of 4905: 1.4201262838691473
Last average training loss at batch 2500 out of 4905: 1.5527204213142396
Last average training loss at batch 3000 out of 4905: 1.6496725532114507
Last average training loss at batch 3500 out of 4905: 1.6906958431452512
Last average training loss at batch 4000 out of 4905: 1.7774249226748944
Last average training loss at batch 4500 out of 4905: 1.456132397353649
Last average training loss at batch 4905 out of 4905: 0.13416633245048173

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 568/702 [01:20<00:18,  7.10it/s][A
                                                 [A 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [29:10:53<1:10:10, 842.12s/it]Average loss of 1.6236319541931152 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 96 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:13<?, ?it/s][A
 13%|â–ˆâ–Ž        | 639/4905 [01:40<11:08,  6.38it/s][A
 26%|â–ˆâ–ˆâ–‹       | 1294/4905 [03:20<09:18,  6.47it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1972/4905 [05:00<07:24,  6.61it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2644/4905 [06:40<05:40,  6.65it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3325/4905 [08:20<03:55,  6.70it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4005/4905 [10:00<02:13,  6.73it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4695/4905 [11:40<00:30,  6.79it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3632156739085912
Last average training loss at batch 1000 out of 4905: 1.3069211908876897
Last average training loss at batch 1500 out of 4905: 1.3555355238765479
Last average training loss at batch 2000 out of 4905: 1.4214285353273153
Last average training loss at batch 2500 out of 4905: 1.5522009010761977
Last average training loss at batch 3000 out of 4905: 1.6499327341169119
Last average training loss at batch 3500 out of 4905: 1.6895562615394593
Last average training loss at batch 4000 out of 4905: 1.7782493516802789
Last average training loss at batch 4500 out of 4905: 1.4554827132672072
Last average training loss at batch 4905 out of 4905: 0.1341331419534032

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:11<?, ?it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 571/702 [01:20<00:18,  7.13it/s][A
                                                 [A 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [29:24:41<55:51, 837.99s/it]  Average loss of 1.6257097721099854 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 97 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:15<?, ?it/s][A
 14%|â–ˆâ–Ž        | 672/4905 [01:40<10:30,  6.72it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1339/4905 [03:20<08:53,  6.69it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1976/4905 [05:00<07:27,  6.54it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2659/4905 [06:40<05:38,  6.64it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3334/4905 [08:20<03:55,  6.68it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3992/4905 [10:01<02:17,  6.64it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4672/4905 [11:41<00:34,  6.69it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362382774963975
Last average training loss at batch 1000 out of 4905: 1.3069813466966151
Last average training loss at batch 1500 out of 4905: 1.358038611277938
Last average training loss at batch 2000 out of 4905: 1.419277561724186
Last average training loss at batch 2500 out of 4905: 1.5523720477074385
Last average training loss at batch 3000 out of 4905: 1.6497805414050817
Last average training loss at batch 3500 out of 4905: 1.690882122978568
Last average training loss at batch 4000 out of 4905: 1.7762186453342437
Last average training loss at batch 4500 out of 4905: 1.4562527001351118
Last average training loss at batch 4905 out of 4905: 0.13422357807890964

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:19<?, ?it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 547/702 [01:20<00:22,  6.79it/s][A
                                                 [A 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [29:38:39<41:53, 837.86s/it]Average loss of 1.6250861883163452 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 98 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:18<?, ?it/s][A
 14%|â–ˆâ–Ž        | 666/4905 [01:40<10:36,  6.66it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1337/4905 [03:20<08:53,  6.68it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1989/4905 [05:00<07:21,  6.61it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2666/4905 [06:40<05:35,  6.67it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3358/4905 [08:20<03:48,  6.76it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4033/4905 [10:00<02:09,  6.75it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4715/4905 [11:40<00:28,  6.77it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3626145065575839
Last average training loss at batch 1000 out of 4905: 1.3048131251186132
Last average training loss at batch 1500 out of 4905: 1.3579309912472963
Last average training loss at batch 2000 out of 4905: 1.4203254043757916
Last average training loss at batch 2500 out of 4905: 1.5514832135736942
Last average training loss at batch 3000 out of 4905: 1.649714640930295
Last average training loss at batch 3500 out of 4905: 1.6908983826190234
Last average training loss at batch 4000 out of 4905: 1.7764511103332044
Last average training loss at batch 4500 out of 4905: 1.455821839749813
Last average training loss at batch 4905 out of 4905: 0.13420759395266163

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:18<?, ?it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 574/702 [01:20<00:17,  7.14it/s][A
                                                 [A 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [29:52:27<27:49, 834.95s/it]Average loss of 1.6254688501358032 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 99 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:19<?, ?it/s][A
 12%|â–ˆâ–        | 577/4905 [01:40<12:32,  5.75it/s][A
 24%|â–ˆâ–ˆâ–       | 1194/4905 [03:20<10:19,  5.99it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1875/4905 [05:00<07:56,  6.36it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2537/4905 [06:40<06:06,  6.46it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3222/4905 [08:20<04:14,  6.60it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3903/4905 [10:00<02:30,  6.67it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4565/4905 [11:40<00:51,  6.65it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.362327326387167
Last average training loss at batch 1000 out of 4905: 1.3051107967942952
Last average training loss at batch 1500 out of 4905: 1.3558863174468279
Last average training loss at batch 2000 out of 4905: 1.4204750816076994
Last average training loss at batch 2500 out of 4905: 1.553073176562786
Last average training loss at batch 3000 out of 4905: 1.6513870470374823
Last average training loss at batch 3500 out of 4905: 1.6894309537112713
Last average training loss at batch 4000 out of 4905: 1.7774624643623829
Last average training loss at batch 4500 out of 4905: 1.4548306554406882
Last average training loss at batch 4905 out of 4905: 0.13430797263114094

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:18<?, ?it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 572/702 [01:20<00:18,  7.13it/s][A
                                                 [A 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [30:06:35<13:58, 838.86s/it]Average loss of 1.6225742101669312 is not an improvement over 1.6154884099960327. New model will not be saved
Starting epoch 100 out of 100

  0%|          | 0/4905 [00:00<?, ?it/s][A
  0%|          | 0/4905 [00:12<?, ?it/s][A
 14%|â–ˆâ–Ž        | 663/4905 [01:40<10:40,  6.63it/s][A
 27%|â–ˆâ–ˆâ–‹       | 1322/4905 [03:20<09:02,  6.60it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2006/4905 [05:00<07:12,  6.71it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2662/4905 [06:40<05:37,  6.65it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3349/4905 [08:20<03:51,  6.72it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3879/4905 [10:01<02:44,  6.23it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4491/4905 [11:41<01:06,  6.19it/s][A
                                                   [ALast average training loss at batch 500 out of 4905: 1.3613308386951686
Last average training loss at batch 1000 out of 4905: 1.3071671721041203
Last average training loss at batch 1500 out of 4905: 1.3565522159188985
Last average training loss at batch 2000 out of 4905: 1.420697484448552
Last average training loss at batch 2500 out of 4905: 1.5520447412133216
Last average training loss at batch 3000 out of 4905: 1.6509492750167847
Last average training loss at batch 3500 out of 4905: 1.6903973210602998
Last average training loss at batch 4000 out of 4905: 1.777437985867262
Last average training loss at batch 4500 out of 4905: 1.455109654545784
Last average training loss at batch 4905 out of 4905: 0.13429153986297981

  0%|          | 0/702 [00:00<?, ?it/s][A
  0%|          | 0/702 [00:10<?, ?it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 590/702 [01:20<00:15,  7.33it/s][A
                                                 [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [30:20:51<00:00, 843.99s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [30:20:51<00:00, 1092.52s/it]
Average loss of 1.6226552724838257 is not an improvement over 1.6154884099960327. New model will not be saved
Time to fit: 1 day, 6:20:51.640420
Training Completed
