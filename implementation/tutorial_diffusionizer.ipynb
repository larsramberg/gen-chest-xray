{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13c9bbf-7be7-4df2-851b-8339cf0fbbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/larsira/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# The implementation here is based on the butterfly example from the hugging face documentation: https://huggingface.co/docs/diffusers/v0.19.2/tutorials/basic_training\n",
    "# The implementation follows the  \n",
    "\n",
    "from torch import nn, optim, float32, LongTensor, uint8\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from util.plot_tools import show_and_save\n",
    "from diffusion.sampling import sample_single_image\n",
    "from dataset.chestxray import ChestXRayDataset\n",
    "from datahandling.dataloader import get_list_from_txt, extract_annotation_targets, extract_unique_labels, extract_n_single_label_images, extract_n_images_from_labels\n",
    "from datahandling.transforms import to_numeric_label, to_class_int\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models\n",
    "from models.diffusers import cxr_unet\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import ast\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from diffusers import DDPMScheduler\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326cc1ae-c506-4366-ab19-29936f4a76f1",
   "metadata": {},
   "source": [
    "# Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4628a6d0-49bb-47c0-b16e-7da15e369149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda test_list.txt\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "debug = ast.literal_eval(os.getenv(\"DEBUG\"))\n",
    "db_path = os.getenv(\"DB_PATH\")\n",
    "img_dir_name = os.getenv(\"IMG_DIR\")\n",
    "class_file_name = os.getenv(\"CLASSIFICATION_FILE\")\n",
    "train_list = os.getenv(\"TRAIN_VAL_LIST\")\n",
    "test_list = os.getenv(\"TEST_LIST\")\n",
    "\n",
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(device, test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385e5f1-8ebc-426b-ac9b-b81c2a4ccd22",
   "metadata": {},
   "source": [
    "# Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f202dc75-515c-43f3-ac38-57181f06a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_TIMESTEPS = 1000\n",
    "IMG_SIZE = 128\n",
    "NUM_GENERATE_IMAGES = 9\n",
    "BATCH_SIZE=2\n",
    "WARMUP_STEPS=100\n",
    "NUM_EPOCHS=100\n",
    "LEARNING_RATE= 1e-4\n",
    "MIXED_PRECISION=\"fp16\"\n",
    "GRADIENT_ACCUMULATION_STEPS=1\n",
    "NUM_TIMESTEPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837de9f3-b2dd-4d31-9519-866f048bbe36",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9868fb48-e356-42c9-9cc1-bcf2e6982471",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = os.path.join(db_path, img_dir_name)\n",
    "annotations_file = os.path.join(db_path, class_file_name)\n",
    "target_file = os.path.join(db_path, \"small_file.csv\")\n",
    "annots = extract_n_images_from_labels(annotations_file, 500, [ChestXRayDataset.target_labels[0], \"Mass\"], target_file, True)\n",
    "\n",
    "preprocess_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],[0.5]),\n",
    "])\n",
    "\n",
    "train_dataset = ChestXRayDataset(target_file, img_dir, transform=preprocess_transforms, read_lib=\"pil\", target_transform=to_class_int)\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f154d54-2f9f-4268-90da-4df8bec44bca",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b6f8bd-768d-49b7-be50-5c4cc99655db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cxr_unet(IMG_SIZE, len(ChestXRayDataset.target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627df57-a426-4379-ac87-5615d48f9042",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eab2720-66e3-44b2-aa18-292fc52840c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([880, 427], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:00<03:32,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([494, 522], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:00<03:15,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([416, 628], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [00:01<03:06,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([614,  95], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [00:01<03:02,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([ 16, 804], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [00:01<02:59,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([664,  99], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/500 [00:02<02:58,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([965, 336], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [00:02<03:17,  2.49it/s]\n",
      "  0%|          | 0/100 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0], device='cuda:0', dtype=torch.int32)\n",
      "tensor([842, 711], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=(len(train_loader) * NUM_EPOCHS)\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=MIXED_PRECISION,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n",
    ")\n",
    "\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, lr_scheduler)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in tqdm(range(NUM_EPOCHS), position=0, leave=True):\n",
    "    model.train()\n",
    "    train_running_loss=0\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        clean_images=batch[0].to(device)\n",
    "        labels=batch[1].flatten().to(device)\n",
    "        clean_images.to(device)\n",
    "        noise = torch.randn(clean_images.shape).to(device)\n",
    "        last_batch_size=len(clean_images)\n",
    "\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (last_batch_size,)).to(device)\n",
    "        print(labels)\n",
    "        print(timesteps)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False, class_labels=labels)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        train_running_loss += loss.item()\n",
    "    train_loss = train_running_loss / (idx+1)\n",
    "\n",
    "    train_learning_rate = lr_scheduler.get_last_lr()[0]\n",
    "    print(\"-\"*30)\n",
    "    print(f\"train loss epoch: {epoch+1}: {train_loss:.4f}\")\n",
    "    print(f\"Train Learning Rate EPOCH: {epoch+1}: {train_learning_rate}\")\n",
    "    if epoch%10 == 0:\n",
    "        img_1 = sample_single_image(model, IMG_SIZE, device, NUM_TRAIN_TIMESTEPS, ChestXRayDataset.target_labels[0], ChestXRayDataset.target_labels, epoch)\n",
    "        show_and_save(img_1, \"Class: {}, Epoch: {}\".format(ChestXRayDataset.target_labels[0], epoch), \"result/{}_{}\".format(ChestXRayDataset.target_labels[0], epoch))\n",
    "        img_2 = sample_single_image(model, IMG_SIZE, device, NUM_TRAIN_TIMESTEPS, \"Mass\", ChestXRayDataset.target_labels, epoch)\n",
    "        show_and_save(img_2, \"Class: {}, Epoch: {}\".format(\"Mass\", epoch), \"result/{}_{}\".format(\"Mass\", epoch))\n",
    "        # sample_image_generation(model, noise_scheduler, NUM_GENERATE_IMAGES, RANDOM_SEED, NUM_TIMESTEPS)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217e76f-57f4-49a2-a098-da366be4e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_single_image(model, IMG_SIZE, device, NUM_TRAIN_TIMESTEPS, ChestXRayDataset.target_labels[0], ChestXRayDataset.target_labels, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b5ae9-97f6-48e4-bd19-1685f9750119",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_single_image(model, IMG_SIZE, device, NUM_TRAIN_TIMESTEPS, \"Mass\", ChestXRayDataset.target_labels, 1001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
